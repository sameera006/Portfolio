[
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.1 SUMMARY",
    "text": "5.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geo-spatial data sets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (beginner?)\n\n\nThis weeks practical was really interesting as we got to use GEE and all the things we did in previous week can be done in google earth engine too with just few lines of code and without downloading any image.\nWe can make a new point and center the map to it in the code editor.\n\nSo this code chunk will center the point to Delhi. For loading the imagery either Landsat or sentinel-1 or any other we can simply browse it and and load the data. then we can add different layers, create a filter based on image and reduce it using median reducer. Mosaic the images, clip it and calculate different textures like Principal Component Analysis (PCA) and also calculate indices like NDVI.\n\n\n\nPCA Layer\n\n\n\n\n\nNDVI"
  },
  {
    "objectID": "Week6.html#application",
    "href": "Week6.html#application",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.2 APPLICATION",
    "text": "5.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geo spatial analysis software platforms” (Ballinger 2024)\n\n\n\n\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (Xie et al. 2019). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nThere are very clear advantages of GEE with some limitation too as its a web based application.\n\n\n\n\n\n(Amani et al. 2020)\n(Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform. While according to (Hansen et al. 2013), it took 100 h to process 654 178 Landsat-7 images (707 terabytes) within GEE and produce a global map of forests otherwise without this the process would have taken a million hours to complete. Meanwhile we are talking about the advantages of using GEE another paper from (Kumar and Mutanga 2018) has done his research that see in many developing countries, the trend of publishing research output using GEE in most disciplines drops behind that for the developed nations."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.3 REFLECTION",
    "text": "5.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregate function like how to use the reduce function and calculate the mean, median and standard variation of the stack of images.\nThe platform has a learning curve, especially for those who haven’t worked with JavaScript before. Additionally, it appears that extra code is necessary to add features like legends, as opposed to the more straightforward button-click approach in QGIS. Another point of confusion is that one must execute the entire script instead of running relevant code chunks individually. Hopefully, future updates will streamline this process.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” January 10, 2024. https://oballinger.github.io/CASA0025/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nHansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, et al. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (6160): 850–53. https://doi.org/10.1126/science.1244693.\n\n\nKumar, Lalit, and Onisimo Mutanga. 2018. “Google Earth Engine Applications Since Inception: Usage, Trends, and Potential.” Remote Sensing 10 (September): 1509. https://doi.org/10.3390/rs10101509.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489.\n\n\nXie, Zunyi, Stuart R. Phinn, Edward T. Game, David J. Pannell, Richard J. Hobbs, Peter R. Briggs, and Eve McDonald-Madden. 2019. “Using Landsat Observations (1988–2017) and Google Earth Engine to Detect Vegetation Cover Changes in Rangelands - A First Step Towards Identifying Degraded Lands for Conservation.” Remote Sensing of Environment 232 (October): 111317. https://doi.org/10.1016/j.rse.2019.111317."
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nCASA0023 Remotely Sensing Cities and Environments is about remotely sensed earth observation(EO) data to make some informed decisions on environmental hazards arising from changing climate.\nWhat is remote sensing?\nAccording to NASA, Remote Sensing is acquiring information from a distance. The data collection may take place directly in the field (in-situ or in-place data collection), and/or at some remote distance from the subject matter known as Remote Sensing of the environment.\nExample- the study of daily weather and climate change, land-use/land cover monitoring, food security, military reconnaissance, and many others.\nThe majority of remotely sensed data are analyzed using digital image processing techniques. A very interesting fact about remote sensing image interpretation is that it is both an art and a science.\n\n\n\nRemote Sensing Process. (CRISP,2001, n.d.)\n\n\nSpace junk, or space debris, is any piece of machinery or debris left by humans in space. It can refer to big objects such as dead satellites that have failed or been left in orbit at the end of their mission.\n\n\n\nSpace Junk. (MIT 2017)\n\n\nSpectral Resolution is the number and dimension (size) of wavelength intervals (referred to as bands or channels) in the electromagnetic spectrum to which a remote sensing instrument is sensitive.\nSpatial Resolution is a measure of the smallest angular or linear separation between two objects that can be resolved by the remote sensing system.\nTemporal Resolution The temporal resolution of a remote sensing system generally refers to how often and when the sensor records imagery of a particular area.\nThe Hot topics being discussed around the world like urban green space and accessibility, Illegal logging, Forest Fire, and temperature studies.\nDifferent types of satellite imagery-\nSentinel 2 -The main advantage of Sentinel 2 over Landsat 8 is its higher resolution (across most bands, it has higher spatial resolution, its revisit time is shorter, and it has more spectral bands). However, Landsat 8 has thermal infrared bands, which means that it can be used to measure temperature.\nLandsat 8 –its history like the origin of satellite imagery USGS -Landsat 8 has a mission to collect global data and give scientists the ability to access changes in Earth’s landscape.\nRemote sensing is performed using an instrument, often referred to as a sensor.\nSensors\nPassive sensors record electromagnetic radiation that is reflected or emitted from the terrain. For example, cameras and video recorders can be used to record visible and near-infrared energy reflected from the terrain.\nand Active sensors such as microwave (RADAR), LiDAR, or SONAR, bathe the terrain in machine-made electromagnetic energy and then record the time-lapsed amount of radiant flux scattered back toward the sensor system.\nRemote sensor data are collected passively (e.g. digital cameras) or actively (e.g., RADAR, LiDAR) using analog or digital remote sensing instruments.\nAtmospheric scattering\n\nAtmospheric scattering causes the colors we see in the sky due to sunlight interacting with particles in the atmosphere.\nRayleigh scattering explains how light interacts with small particles, resulting in blue skies during the day.\n\n\n\n\nRayleigh Scattering (“Rayleigh Scattering” 2024)\n\n\nThis week practical we did play with Sentinel and Landsat imagery and compared it with different classes like urban, grass, bare-earth. We compare the spectral signatures from both. We can do this by generating point of interest (POIs) matches with both images. We can see below in the figure\n\n\n\nSentinel-1 and Landsat image of South Africa"
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote sensing plays a crucial role in modern agriculture by providing data-driven insights. It aids in recent yield estimation,facilitates monitoring crop health and development, Detecting infestations remotely.(Shah et al. 2023). Despite advancements in sensor technologies, the agricultural sector faces knowledge gaps regarding the sufficiency, appropriateness, and techno-economic feasibility of remote sensing (RS) technologies.\nFurther improving the use of remote sensing and making it more effective can be seen in this paper which presents a method using optical remote sensing and machine learning (decision trees and random forests) to map cropland, cropping patterns, and crop types. It outlines steps including preprocessing, model training, and validation. While effective, considerations include algorithm justification, robust accuracy assessment, and ensuring applicability across regions. It is important to generalize it and use the ground truth data for credibility and application of methodology.(Tariq et al. 2023)"
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nWhen I think about remote sensing the first thing that comes to my mind is high-resolution images captured by satellites and we humans working on it. Be it land, water, and events like fire, floods, earthquakes, any natural disaster, or man-made disasters like war which includes bombing and destruction of houses. Everything can be recorded, and we can study it, and be aware of it for the future. It’s just like magic. Last week in the Big Data class I was fascinated to see how daily images of Gaza were available and we can see how much destruction has happened or is happening. Remote sensing gives you time-series data through which we can tell how the place has changed in all these years. How many buildings got demolished, how many trees were cut down, and how many new buildings were constructed? In my past life, I have used remotely sensed data to classify land use land-cover and generate environmental impact assessment reports for work. We also studied atmospheric correction which sounds similar to something we did in our GIS class last term. The imagery downloaded can be cleaned just like we clean data or use .csv or .shp files for GIS work. We can remove all the bad layers. Run the software as many times to get the best clean image. It’s like removing all the NaNs. To conclude in more general terms the entire industry is dependent on high-resolution satellite imagery and open-source applications like Google Earth Engine, Google Maps, open-street maps.\n\n\n\n\nCRISP,2001. n.d. “Principles of Remote Sensing - Centre for Remote Imaging, Sensing and Processing, CRISP.” https://crisp.nus.edu.sg/~research/tutorial/optical.htm.\n\n\nMIT. 2017. “Space Junk: The Cluttered Frontier.” https://news.mit.edu/2017/space-junk-shards-teflon-0619.\n\n\n“Rayleigh Scattering.” 2024. https://en.wikipedia.org/w/index.php?title=Rayleigh_scattering&oldid=1213539014.\n\n\nShah, Adnan Noor, Muhammad Adnan Bukhari, Zahoor Ahmad, Asad Abbas, Abdul Manan, Muhammad Umair Hassan, Muhammad Saqib, et al. 2023. “Application of Remote Sensing in Agriculture.” In, edited by Wajid Nasim Jatoi, Muhammad Mubeen, Muhammad Zaffar Hashmi, Shaukat Ali, Shah Fahad, and Khalid Mahmood, 371–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-26692-8_21.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and Faisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and Crop Types by Combining Optical Remote Sensing Images with Decision Tree Classifier and Random Forest.” Geo-Spatial Information Science 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287."
  },
  {
    "objectID": "Week2.html#xaringan-presentation",
    "href": "Week2.html#xaringan-presentation",
    "title": "2  Xaringan",
    "section": "2.1 Xaringan Presentation",
    "text": "2.1 Xaringan Presentation"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Corrections",
    "section": "3.1 SUMMARY",
    "text": "3.1 SUMMARY\nThis week we looked at the story of Virginia Norwood known as the mother of Landsat. She designed the Multispectral Scanner (MSS) against the good old RBV(return beam vidicon) sensor.\nLandsat collects images in long narrow strips called “swaths.” Previous Landsat sensors swept back and forth across the swath like a whisk broom to collect data.\nIn contrast, the instruments on Landsat 8 view across the entire swath at once, building strips of data like a push broom.\n\nGEOMETRIC CORRECTION\nGeometric correction caused by sensor system attitude (roll, pitch, and yaw) or altitude changes can be corrected by ground control points(GCP).(Nguyen 2015)\nThere are two types of geometric correction-\n\nimage-to-image rectification and\nimage-to-image registration\n\n\n\n\n\n\n\n\n\nExample of image-to-image hybrid registration (Jensen 2015)\n\n\nATMOSPHERIC CORRECTION\nThe two most important sources of environmental attenuation are\n1) atmosphere attenuation caused by scattering and absorption in the atmosphere, and 2) topographic attenuation.\nFor example, consider the case of the normalized difference vegetation index (NDVI) derived from Landsat Thematic Mapper (TM ) band 3 (red) and band 4 (near-infrared) data. A rather simple method to correct raw satellite (or any other imagery) is called dark object subtraction(DOS) which uses the idea that the darkest pixel within the image should be 0 and any value it has is attributed to atmosphere. So to remove it we will subtract that value from the rest of the pixels within the image.\nIn the practical we have selected two tiles for Cape Town. The image below shows the NDVI for Cape Town using Landsat imagery from 2022.\n\n\n\nNDVI values -Cape Town, South Africa\n\n\nThe focus for this weeks practical was on image enhancements such as band rationing, filter, texture, data fusion, and Principal Component Analysis (PCA).\nIn band rationing, the contrast between features are enhanced by “dividing the brightness values (digital numbers) in a reflectance curve after the additive atmospheric haze and additive sensor offset have been removed”.\nHere we have taken a smaller study area polygon from the original image in order to reduce the processing time. As running texture analysis in R takes quite a time.\n\n\n\nTexture Analysis of the study area from CapeTown\n\n\nPCA is basically common data reduction technique. Its is used to reduce the number of bands of multi-spectral imagery. Below is the example from this weeks practical.\n\n\n\nPCA of the study area from CapeTown"
  },
  {
    "objectID": "Week3.html#application",
    "href": "Week3.html#application",
    "title": "3  Corrections",
    "section": "3.2 APPLICATION",
    "text": "3.2 APPLICATION\nThis application section will focus on studies that have made use of atmospheric corrections. In this theme (Nazeer et al. 2021), the atmospheric correction study shows that the Simplified and Robust Surface Reflectance Estimation Method (SREM) indeed presents a promising approach to atmospheric correction in satellite remote sensing data. Its simplicity, not requiring inputs like Aerosol Optical Depth (AOD), aerosol type, water vapor, and ozone, is a significant advantage. But the author should consider the assumptions are true for all conditions. Secondly validation of dataset is very crucial step. And also comparing with other models provides a more comprehensive evaluation of SREM’s effectiveness. Similarly (Dong et al. 2023) study presents a compelling case for the use of the Sensor Invariant atmospheric correction (SIAC) method in correcting atmospheric effects in wide-swath GF-1 and GF-6 WFV images. The improvement in accuracy for both types of images is a significant achievement. However some limitations includes this method can be used to other types of images basically how can we make it more generalized and validation checking with other models is crucial and incorporating reflectance measurements obtained during a field campaign in the future."
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Corrections",
    "section": "3.3 REFLECTION",
    "text": "3.3 REFLECTION\nWell, this week was quite interesting and intense as we learned a lot of different techniques to improve our imagery. What I understood is that when we download imagery from the satellite onto our computer the first step to use it in our analysis is to check its characteristics like the DN, and radiance value. After that, we take measures like atmospheric correction, geometric correction, fusion, and other techniques that improve the quality of imagery. Usually these days if we download any Landsat imagery or Sentinel-1 most of the corrections like DN or radinace value is already been taken care of so we do not have to check the above but it is a good practice to know all these specifics about the satellite imagery. We can always use image fusion or image enhancement and even change the texture. Well that is really cool.\n\n\n\n\nDong, Yi, Wei Su, Fu Xuan, Jiayu Li, Feng Yin, Jianxi Huang, Yelu Zeng, Xuecao Li, and Wancheng Tao. 2023. “An Effective Atmospheric Correction Method for the Wide Swath of Chinese GF-1 and GF-6 WFV Images on Lands.” The Egyptian Journal of Remote Sensing and Space Sciences 26 (3): 732–46. https://doi.org/10.1016/j.ejrs.2023.07.011.\n\n\nJensen, John R. 2015. Introductory Digital Image Processing. 4th ed. A Remote Sensing Perspective. Pearson Higher Education US.\n\n\nNazeer, Majid, Christopher Olayinka Ilori, Muhammad Bilal, Janet Elizabeth Nichol, Weicheng Wu, Zhongfeng Qiu, and Bijoy Krishna Gayene. 2021. “Evaluation of Atmospheric Correction Methods for Low to High Resolutions Satellite Remote Sensing Data.” Atmospheric Research 249 (February): 105308. https://doi.org/10.1016/j.atmosres.2020.105308.\n\n\nNguyen, Thanh. 2015. “Optimal Ground Control Points for Geometric Correction Using Genetic Algorithm with Global Accuracy.” European Journal of Remote Sensing 48 (1): 101–20. https://doi.org/10.5721/EuJRS20154807."
  },
  {
    "objectID": "Week4.html#summary",
    "href": "Week4.html#summary",
    "title": "4  Policy",
    "section": "4.1 SUMMARY",
    "text": "4.1 SUMMARY\nHealthy land is central to the well being of the planet’s ecosystems and biodiversity; it feeds us, shelters us, and provides the backbone to a thriving global economy. The United Nations Convention to combat Desertification (UNCCD) was established in 1994 to protect and restore our land and ensure a safer, just, and more sustainable future.\nClimate-induced desertification is having adverse effects on the African continent each year the Earth continues to warm. It impacts the everyday lives of Africans – from their crops, livestock, and housing – to African wildlife and biodiversity. (Stallwood 2022)\nAccording to European Commission world atlas of Desertification\n\nDegraded Land: Over 75% of Earth’s land area is already degraded, and more than 90% could face degradation by 2050.\nGlobal Impact: Annually, an area equivalent to half the size of the European Union (about 4.18 million km²) experiences degradation, with Africa and Asia being the most affected regions.\nCrop Yields: Land degradation and climate change may reduce global crop yields by 10% by 2050, particularly in India, China, and sub-Saharan Africa.\nDisplacement: By 2050, up to 700 million people could be displaced due to scarce land resources, potentially reaching 10 billion by the century’s end.\nLocal Solutions: Addressing land degradation requires local solutions and stronger cooperation at the local level to preserve biodiversity and combat this global challenge(2019?)\n\n\nThe importance of land degradation and desertification led to the adoption of Sustainable Development Goal 15.3 aiming at land degradation neutrality.\nIn 2007, an initiative called “The Green Wall” was launched for the Sahara and Sahel regions. Its ambitious goal is to create an 8,000-kilometer natural wonder spanning the entire width of Africa. By planting more trees, this project aims to combat desertification, boost food security, create jobs, and encourage migrants to return home to Africa.\nBy 2030, The Great Green Wall aims to restore 247 million acres of degraded land and generate 10 million jobs in affected rural areas. The AFR100 initiative’s goal of restoring 100 million hectares by 2030 is more achievable than it may seem, especially considering the recent $14 billion funding commitment for the Great Green Wall at the One Planet Summit for Biodiversity. (Stallwood 2022)\n\n\n\nFigure 2: The Eleven Nations that are investing in the Great Green Wall Initiative _@National Geographic (Jose 2023)\n\n\nThe city of my interest is Naimey, the capital city of Niger which is experiencing the issue of desertification. Since 1960, the population of Niger has increased from 1.7 million to 17 million. Mostly are farmers and herders. these people place huge pressure on the vegetation. It is estimated 50 million people globally could be displaced by desertification over the next decade.\nOne of the main problems what i found was to track the project because there is not a good monitoring and evaluation system in place."
  },
  {
    "objectID": "Week4.html#application",
    "href": "Week4.html#application",
    "title": "4  Policy",
    "section": "4.2 APPLICATION",
    "text": "4.2 APPLICATION\nDesertification monitoring often uses vegetation index thresholds to assess land degradation.(Bezerra et al. 2020) Evidently, this method only considers vegetation cover and ignores soil information, leading to the lower accuracy of the classification results. (Wei et al. 2018)\nTo solve this ongoing issue we can come up with some better solutions like instead of just calculating NDVI we should also add NDWI and SAVI index and see the how the soil and water factors this problem. Additionally we can see how population and land is affected. As the policy is already there i think we just need people to think about this situation and involve methods which can boost up the GGW initiative. We can also identify areas where trees can be planted.\n\n\n\n\n\nThe following steps are involved in the methodology -\n 1. Vegetation index analysis: Analyse the loss of green, degradation of lands in Iraq over the years (using NDVI, NDWI, and other indices).\n2. Drought area analysis: Use supervised machine learning algorithms to classify different land type covers.\n3. Times series analysis: Analyse and predict the desertification process in the next few years through the historical data.\n4. LandUse Land Cover Classification using machine learning with Google Earth Engine: Using the cloud power provided by Google Earth Engine to train a land cover classifier.\n5. Dashboard: Build a dashboard to visualize the areas affected and our future predictions. (zotero-315?)"
  },
  {
    "objectID": "Week4.html#reflection",
    "href": "Week4.html#reflection",
    "title": "4  Policy",
    "section": "4.3 REFLECTION",
    "text": "4.3 REFLECTION\nThe big learning point for me this week was to learn about so many different policies and yet we see the issues and problem still remains. It has also highlighted how remote sensing data can be used to address multiple factors in a city for a common goal.\nThe application of remote sensing assists policymakers in creating informed maps for desertification decisions. While valuable, it’s not a magic solution. Consider stakeholders, potential repercussions, and ongoing monitoring. Challenges exist, including data complexity. Collaboration is key to maximizing its impact in policy.\n\n\n\n\nBezerra, Francisco Gilney, Ana Paula Aguiar, Regina Alvalá, Angelica Giarolla, Karine Rocha Aguiar Bezerra, Patricia Lima, Flávio Nascimento, and E. Arai. 2020. “Analysis of Areas Undergoing Desertification, Using EVI2 Multi-Temporal Data Based on MODIS Imagery as Indicator.” Ecological Indicators 117 (October): 106579. https://doi.org/10.1016/j.ecolind.2020.106579.\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green Wall, Africa.” RTF | Rethinking The Future. November 3, 2023. https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\nStallwood, Paige. 2022. “Desertification in Africa: Causes, Effects and Solutions.” Earth.Org. December 15, 2022. https://earth.org/desertification-in-africa/.\n\n\nWei, Haishuo, Juanle Wang, Kai Cheng, Ge Li, Altansukh Ochir, Davaasuren Davaadorj, and Chonokhuu Sonomdagva. 2018. “Desertification Information Extraction Based on Feature Space Combinations on the Mongolian Plateau.” Remote Sensing 10 (October): 1614. https://doi.org/10.3390/rs10101614."
  },
  {
    "objectID": "Week5.html#summary",
    "href": "Week5.html#summary",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geospatial datasets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (“Beginner’s Cookbook | Google Earth Engine,” n.d.)"
  },
  {
    "objectID": "Week5.html#application",
    "href": "Week5.html#application",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms” (Ballinger 2024)\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (xie2019?). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nLong et al. (Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform."
  },
  {
    "objectID": "Week5.html#reflection",
    "href": "Week5.html#reflection",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregrate function like hoe to use the reduce function and calculate the mean, median and standard variation of the stack of images.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\n“Beginner’s Cookbook | Google Earth Engine.” n.d. https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "6  Classification-1",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nClassification can be used in many studies like Urban Expansion using Landsat data, Air pollution, Urban green Spaces, monitoring forest like illegal logging, and Forest fires. in all these studies land cover is extracted from the earth observation data.\nSo, the process of classification involves inductive learning which means that by looking at the map we know where is ground or barren land or urban or forest area and accordingly we can classify.\nFor classification we use an expert system.\n\n(whatis?)\nSo the expert system consist of the knowledge base and inference engine. what we want to find out how a computer replicate human knowledge. and how expert system is applied to remote sensing.\nMachine learning is defined as the science of computer modeling of learning process. there are number of machine learning data decision tree /regression tree.\nCART (Classification and Regression Tree)\nClassification tree classify data into two or more discrete categories. For example Land cover. Its a tree-like graph with nodes and leaves. Another example we can take is if we want to play badminton which depend on the conditions outside hot or cold, the speed of wind and the weather.\n\n\n\n\n\n\n\n\n\n\n(decision?)\nRegression trees predict continuous dependent variable like numerical variable for example total population. They subset the data into smaller chunks. When we create decision tree the end leaves might be a mixture of categories meaning they are impure known as Gini impurity. The option with the lowest impurity goes to the top of the tree and becomes the root.\nWhile discussing model prediction it is important to understand prediction errors(bias and variance). so it is very important to understand these errors which will help us build accurate models and avoid the mistake of over fitting and under fittingg. (Singh 2018)\nSo decision trees (DTs) are not good with new or big size data. Higher depth DTs are more prone to over fitting and thus leads to higher variance. This shortcoming of DTs is explored by random forest model.\nRandom Forests\nRandom Forests (RFs) are composed of multiple independent decision trees that are trained independently on a random subset of data.\n\n(Bhatia 2019)\nIn RFs we talk about bootstrapping and out of bag(OOB) error which is basically we do not train all the values and what is left is then tested and validated with the model. the left out variables are called OOB.\nhow do we apply these methods with the satellite imagery?\nImage Classification Techniques - it is the process of assigning land cover classes to pixels. For example classes include water, urban, Forest, agriculture and grassland.\n\nSupervised\nUnsupervised\nObject based Image Classification\nUnsupervised classification usually referred as clustering/k-means. some common clustering algorithm are ISODATA.\nSupervised Classification we use Maximum likelihood and support vector machines(SVM).\nThis week’s practical brings us to Shenzhen where we classified data using CART and Random Forest.\n\n\nCART Classifier"
  },
  {
    "objectID": "Week7.html#application",
    "href": "Week7.html#application",
    "title": "6  Classification-1",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“Identifying specific crop types, cropland, and cropping patterns using space-based observations is challenging because different crop types and cropping patterns have similarity spectral signatures. But combining satellite imagery with machine learning methods helps in monitoring the above concern. In this study, Sentinel-2 and Landsat-8 time series data were used with the Decision Tree Classifier (DTC) and Random Forest (RF) methods to map cropland and crop types in Gujranwala’s sub-districts. Optical remote sensing NDVI time series data facilitated cropland mapping, revealing wheat as the predominant crop with 85% coverage, while barley occupied only 0.07% of the total crop area. The analysis focused on areas with consistent agricultural calendars, parcel morphology, and climatic conditions. DTC and RF methods showcased wheat’s superior accuracy.”(Tariq et al. 2023)\n“Remote sensing helps study Urban Green Spaces (UGSs), but less is known about non-tree species, change detection, and health assessment. Most research focuses on large UGSs, neglecting smaller areas like street trees and parks, which are still significant. Urban managers need to identify vegetation species to maintain and protect UGSs from invasive species. Previously, this was costly and hard, relying on field surveys. Now, remote sensing allows for accurate and timely identification of vegetation species in urban areas.”(Shahtahmassebi et al. 2021)\nThe key concerns in the above study is the presence of shadow in high spatial resolution imagery which can reduce the accuracy of UGSs mapping ((Zhang and Qiu 2012); (Merry et al. 2014)). There is also a need to develop methods for extracting informative and intelligent information from Google Street View, for example, species characteristics and the quality of UGSs as might be perceived by users of the spaces.\nA new study suggests creating, testing, and implementing a machine learning approach to assess green space quality based on human perception, using transfer learning from pre-existing models. The findings show that the developed models performed well in six key areas: accuracy, precision, recall, F1-score, Cohen’s Kappa, and Average ROC-AUC. (Rustamov, Rustamov, and Zaki 2023)\n\nUsing remote sensing data, particularly the Normalized Difference Vegetation Index (NDVI), is a common method to evaluate green space quantity on satellite images. (Sun et al. 2021) However, this approach has limitations when assessing green space quality. Remote sensing struggles to detect subtle changes in quality-related variables and provides only a two-dimensional view, which may not accurately represent the green space perceived at eye level. Additionally, while quantity matters, green space quality is more crucial for human health. The complexity of feature extraction in Convolutional Neural Networks (CNN) depends on the number of layers, with fewer layers resulting in weaker learning abilities for complex features.(Ayadi and Lachiri 2022)"
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "6  Classification-1",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nSo, this week was about classification and had a lot of content to cover. I think this is the most important lecture because all the methods taught during the lecture are used very commonly for digital image processing and in future will be used mostly. I understood about decision trees and random forest and how machine learning plays an important role in the field of remote sensing. By week 6 all we did earlier is making sense and helpful in understanding about different applications where we can use this like detecting fires, flood risk, desertification and LULC heat maps.\n\n\n\n\nAyadi, Souha, and Zied Lachiri. 2022. “Deep Neural Network for Visual Emotion Recognition Based on ResNet50 Using Song-Speech Characteristics.” 2022 5th International Conference on Advanced Systems and Emergent Technologies (IC_ASET), March, 363–68. https://doi.org/10.1109/IC_ASET53395.2022.9765898.\n\n\nBhatia, Navnina. 2019. “What Is Out of Bag (OOB) Score in Random Forest?” Medium. June 27, 2019. https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n\n\nMerry, Krista, Jacek Siry, Pete Bettinger, and J. M. Bowker. 2014. “Urban Tree Cover Change in Detroit and Atlanta, USA, 1951–2010.” Cities 41 (December): 123–31. https://doi.org/10.1016/j.cities.2014.06.012.\n\n\nRustamov, Jaloliddin, Zahiriddin Rustamov, and Nazar Zaki. 2023. “Green Space Quality Analysis Using Machine Learning Approaches.” Sustainability 15 (10, 10): 7782. https://doi.org/10.3390/su15107782.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Yue lin, Muye Gan, Ke Wang, Arunima Malik, and George Alan Blackburn. 2021. “Remote Sensing of Urban Green Spaces: A Review.” Urban Forestry & Urban Greening 57 (January): 126946. https://doi.org/10.1016/j.ufug.2020.126946.\n\n\nSingh, Seema. 2018. “Understanding the Bias-Variance Tradeoff.” Medium. October 9, 2018. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229.\n\n\nSun, Yi, Xingzhi Wang, Jiayin Zhu, Liangjian Chen, Yuhang Jia, Jean M. Lawrence, Luo-hua Jiang, Xiaohui Xie, and Jun Wu. 2021. “Using Machine Learning to Examine Street Green Space Types at a High Spatial Resolution: Application in Los Angeles County on Socioeconomic Disparities in Exposure.” The Science of the Total Environment 787 (September): 147653. https://doi.org/10.1016/j.scitotenv.2021.147653.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and Faisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and Crop Types by Combining Optical Remote Sensing Images with Decision Tree Classifier and Random Forest.” Geo-Spatial Information Science 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287.\n\n\nZhang, C., and F. Qiu. 2012. “Mapping Individual Tree Species in an Urban Forest Using Airborne Lidar Data and Hyperspectral Imagery: AAG Remote Sensing Specialty Group 2011 Award Winner.” Photogrammetric Engineering and Remote Sensing 78 (10): 1079–87. https://doi.org/10.14358/PERS.78.10.1079."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achanta, Radhakrishna, and Sabine Susstrunk. 2017. “Superpixels\nand Polygons Using Simple Non-iterative\nClustering.” In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition\n(CVPR), 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing\nPlatform for Remote Sensing Big Data Applications:\nA Comprehensive Review.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nAyadi, Souha, and Zied Lachiri. 2022. “Deep Neural\nNetwork for Visual Emotion Recognition Based on\nResNet50 Using Song-Speech\nCharacteristics.” 2022 5th International Conference on\nAdvanced Systems and Emergent Technologies (IC_ASET), March,\n363–68. https://doi.org/10.1109/IC_ASET53395.2022.9765898.\n\n\nBallinger, Dr Ollie. 2023. “Open Access Damage Detection Using\nSentinel-1 Imagery.” https://oballinger.github.io/PWTT/.\n\n\n———. 2024. “CASA0025: Building Spatial\nApplications with Big Data.” January 10,\n2024. https://oballinger.github.io/CASA0025/.\n\n\nBhatia, Navnina. 2019. “What Is Out of\nBag (OOB) Score in Random\nForest?” Medium. June 27, 2019. https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of\nClassifications of Remotely Sensed Data.” Remote Sensing of\nEnvironment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic\nAperture Radar? | Earthdata.” Backgrounder.\nEarth Science Data Systems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nGISGeography. 2016. “Supervised and Unsupervised\nClassification in Remote Sensing.” GIS\nGeography. June 30, 2016. https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-scale Geospatial Analysis for\nEveryone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools, applications and experiences,\n202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green\nWall, Africa.” RTF | Rethinking The Future.\nNovember 3, 2023. https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022.\n“Spatial Dependence Between Training and Test Sets: Another\nPitfall of Classification Accuracy Assessment in Remote Sensing.”\nMachine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang,\nBingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin.\n2019. “30 m Resolution Global Annual Burned Area Mapping\nBased on Landsat Images and Google Earth\nEngine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489.\n\n\nMerry, Krista, Jacek Siry, Pete Bettinger, and J. M. Bowker. 2014.\n“Urban Tree Cover Change in Detroit and\nAtlanta, USA, 1951–2010.”\nCities 41 (December): 123–31. https://doi.org/10.1016/j.cities.2014.06.012.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith,\nGurutzeta Guillera‐Arroita, Severin Hauenstein, et al. 2017.\n“Cross‐validation Strategies for Data with Temporal, Spatial,\nHierarchical, or Phylogenetic Structure.” Ecography 40\n(8): 913–29. https://doi.org/10.1111/ecog.02881.\n\n\nRustamov, Jaloliddin, Zahiriddin Rustamov, and Nazar Zaki. 2023.\n“Green Space Quality Analysis Using Machine Learning\nApproaches.” Sustainability 15 (10, 10): 7782. https://doi.org/10.3390/su15107782.\n\n\nShah, Adnan Noor, Muhammad Adnan Bukhari, Zahoor Ahmad, Asad Abbas,\nAbdul Manan, Muhammad Umair Hassan, Muhammad Saqib, et al. 2023.\n“Application of Remote Sensing in Agriculture.” In, edited\nby Wajid Nasim Jatoi, Muhammad Mubeen, Muhammad Zaffar Hashmi, Shaukat\nAli, Shah Fahad, and Khalid Mahmood, 371–79. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-031-26692-8_21.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Yue lin, Muye\nGan, Ke Wang, Arunima Malik, and George Alan Blackburn. 2021.\n“Remote Sensing of Urban Green Spaces: A\nReview.” Urban Forestry & Urban Greening 57\n(January): 126946. https://doi.org/10.1016/j.ufug.2020.126946.\n\n\nSingh, Seema. 2018. “Understanding the Bias-Variance\nTradeoff.” Medium. October 9, 2018. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229.\n\n\nStallwood, Paige. 2022. “Desertification in Africa:\nCauses, Effects and\nSolutions.” Earth.Org. December 15, 2022. https://earth.org/desertification-in-africa/.\n\n\nSun, Yi, Xingzhi Wang, Jiayin Zhu, Liangjian Chen, Yuhang Jia, Jean M.\nLawrence, Luo-hua Jiang, Xiaohui Xie, and Jun Wu. 2021. “Using\nMachine Learning to Examine Street Green Space Types at a High Spatial\nResolution: Application in Los Angeles County\non Socioeconomic Disparities in Exposure.” The Science of the\nTotal Environment 787 (September): 147653. https://doi.org/10.1016/j.scitotenv.2021.147653.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and\nFaisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and\nCrop Types by Combining Optical Remote Sensing Images with Decision Tree\nClassifier and Random Forest.” Geo-Spatial Information\nScience 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287.\n\n\nXie, Zunyi, Stuart R. Phinn, Edward T. Game, David J. Pannell, Richard\nJ. Hobbs, Peter R. Briggs, and Eve McDonald-Madden. 2019. “Using\nLandsat Observations (1988–2017) and Google Earth\nEngine to Detect Vegetation Cover Changes in Rangelands -\nA First Step Towards Identifying Degraded Lands for\nConservation.” Remote Sensing of Environment 232\n(October): 111317. https://doi.org/10.1016/j.rse.2019.111317.\n\n\nZhang, C., and F. Qiu. 2012. “Mapping Individual Tree Species in\nan Urban Forest Using Airborne Lidar Data and Hyperspectral Imagery:\nAAG Remote Sensing Specialty Group 2011 Award\nWinner.” Photogrammetric Engineering and Remote Sensing\n78 (10): 1079–87. https://doi.org/10.14358/PERS.78.10.1079."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Learning Diary",
    "section": "",
    "text": "ABOUT\nThis is my online Learning diary.\nIn CASA0023 we are asked to make a portfolio that will help us in the future.\nHi! I am Sameera. I am from India and have lived in many different parts of the country. When someone ask me where in India you are from i have to think and reply as i was born in Kanpur, completed my school there and for further studies I moved to Delhi, the capital of India. and Past five years i have been moving to different cities from Ahmedabad to Kochi to Bangalore. Now here i am in London.\nReading maps has fascinated me ever since I attended cartography lessons back in school; however, I never knew that I would find my calling in the same field in the future. It is exhilarating how I followed my passion and entered the field of GIS after spending over six years working as a lecturer at an undergraduate college.\nI love travelling. My favorite destination till date is Leh, Ladakh. Here’s the picture from that place."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "7  Classification- The Big Question",
    "section": "7.1 SUMMARY",
    "text": "7.1 SUMMARY\nThere is some Pre-classified data which can be used where we don’t need to do the classification. For example Dynamic World Data which use SR for labelling but use TOA for the model. here we use Machine learning models like Convolution Neural Network. Accuracy is assessed through Confusion Matrix.\nWhen we use such classified data we check that how was it trained. There are some issues like blobbiness as its a 50x50 m image.\nOBJECT BASED IMAGE ANALYSIS (OBIA)\nHere we talk about pixels and distance mainly Euclidian distance. OBIA is similarity between pixels. It tries to match the similar pixels. for example - Take a RGB near-infrared 1m image. Its a typical kind of image where we want to do segmentation and OBIA. we do OBIA to do clean classification. Like if we classify on the mean of all the vectors inside a particular polygon we will get a solid result as opposed to classifying on individual pixels we will get a lot salt and pepper noise.\nThere comes also a question why do we need object based analysis? It is to improve in signal and noise. There are thousand pixels here so we might want to turn these things into objects.\n\n\n\n\n\n\n\n\nOriginal Image to Classify\nDifferent Crop Types\nPer Pixel Classification\n\n\n\n\n\n\n\n\n\n\nFig1: Process of Classification\nWe use mode reducer in neighborhood to clean the image. it will remove the single pixel outliers. we also talk about spectral gradient and can we use super-pixels to remove the salt and pepper noise.\n\nFig2: Spectral Gradient and Distances\nSNIC- (Super pixel non-iterative clustering) Seeded region growing\nThis makes clusters without using k-means. It uses a regular grid of points (like k-means) but then assigns pixels to points through distance color and co-ordinates - it represents normalised spatial and color distances. we also have seed grid which denotes spacing in pixel. we can set square or hex grid.\nImage.reduceConnectedComponents\n(Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the most common method regular points on the image work out spatial distance (from point to centre of pixel) = closeness to centre colour difference (RGB vs RGB to centre point) = homogenity of colours\nThe SLIC algorithm works iteratively, repeating the above process until it reaches the expected number of iterations.\nsub pixel analysis/ spectral mixture analysis\nSupervised Classification in Remote Sensing\nIn supervised classification, you select training samples and classify your image based on your chosen samples. Your training samples are key because they will determine which class each pixel inherits in your overall image.\n\nFig3: Classification and Validation(GISGeography 2016)\nThe good approach is to train and test split\n\nThis is simply holding back a % of the original data used to train the model to then test it at the end\n\n\nFig.4: Testing and Training the ML model\nIn the practical, we did sub-pixel, object-based, image and super pixels classification. Below is the sub-pixel classification output for DaresSalaam, Tanzania, using Landsat 8 data.\n\n\n\n\n\nThe question we can think at this stage is about spatial autocorrelation and is OBIA is dealing with spatial autocorrelation?\nIn ML model if we use spatial data and they have not considered spatial autocorrelation between its training and testing data then its considered too good or over-fitted. To solve this we do spatial cross validation."
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "7  Classification- The Big Question",
    "section": "7.2 APPLICATION",
    "text": "7.2 APPLICATION\nThe researchers lately have diverted themselves to a much simpler task of simplifying an image into a smaller cluster of pixels called super pixels than using traditional segmentation algorithms. Several applications such as object localization, multi-class segmentation, optical flow, body model estimation, object tracking, depth estimation took advantage of super pixels.\nThe algorithm to do this is Simple linear Iterative Clustering Algorithm(SLIC) but it has some limitations. It requires several iterations for the centroids to converge. It uses a distance map of the same size as the number of input pixels, which amounts to significant memory consumption for image stacks or video volumes. Lastly, SLIC enforces connectivity only as a post-processing step. So the better version of this is simple non-iterative clustering (SNIC). It is non-iterative, explicitly enforces connectivity, is computationally cheaper, uses lesser memory.(Achanta and Susstrunk 2017)\nSpatial autocorrelation is inherent to remotely sensed data. nearby pixels are more similar than distant ones. The evaluation of classification accuracy has always been considered as an important issue in the remote sensing community. (Congalton 1991)\nDespite the importance attached to the accuracy assessment protocol, a gap, sometimes a serious one, is often found between the performance metrics of a model and the real quality of the resulting map. This tends to reduce the confidence placed in accuracy statistics and to discredit the true capacity of remote sensing in the opinion of the t end users. Because of spatial autocorrelation, spectral values of close pixels are often more similar than those of distant ones, producing falsely high accuracy metrics if the sampling design is not used for testing. (Roberts et al. 2017)"
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "7  Classification- The Big Question",
    "section": "7.3 REFLECTION",
    "text": "7.3 REFLECTION\nThis week we discussed about object based image analysis i think is very important part of classification. The issues related to that and if for example we have spatial data then we always have spatial autocorrelation. They are kind of related . So we need to keep a check on that which can be resolved through spatial cross validation or object based image analysis. If we are using machine learning models for spatial data we need to take care of over-fitting of data and how can we make the more generalized model which can be use for different imagery. this week also talks about lots of methods we discussed in GIS module like Moran’s I, K-means Clustering and GWR. So here we see the merge of Remote Sensing and GIS.\n\n\n\n\nAchanta, Radhakrishna, and Sabine Susstrunk. 2017. “Superpixels and Polygons Using Simple Non-iterative Clustering.” In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data.” Remote Sensing of Environment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nGISGeography. 2016. “Supervised and Unsupervised Classification in Remote Sensing.” GIS Geography. June 30, 2016. https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera‐Arroita, Severin Hauenstein, et al. 2017. “Cross‐validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881."
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.1 SUMMARY",
    "text": "8.1 SUMMARY\nSynthetic Aperture Radar (SAR) data\n“SAR instruments send pulses of microwaves toward Earth’s surface and listen for the reflections of those waves. The radar waves can penetrate cloud cover, vegetation, and the dark of night to detect changes that might not show up in visible light imagery. When Earth’s crust moves due to an earthquake, when dry land is suddenly covered by flood water, or when buildings have been damaged or toppled, the amplitude and phase of radar wave reflections changes in those areas and indicates to the satellite that something on the ground has changed.”\nIt is an active sensor which means a sensor which provides their own illumination. For example it sees the world as the way bats do as they emit a chirp and listen to the backscatter of an echo if something gets reflected while passive sensor (optical) sees the world as our eyes or camera does.\nRadar sensor picks up the signal reflected backscattered from the earth surface. They check the topology of the surface based on how it reflects.\nInterpreting Radar Images\n\nRegions of calm water and other smooth surfaces appear black, because the radar pulse reflects away from the spacecraft\nRough surfaces appear brighter, as they reflect the radar in all directions, and more of the energy is scattered back to the antenna. \n\nDifferent wavelengths penetrates through different surfaces used in different applications. So choice of wavelenghth is important.\n\n\n\n\n\n(Earth Science Data Systems 2020)\nThe most commonly used Band in imaging is C-band with 4-8 GHz frequency.\nPolarization\nImaging radars have different polarization configurations. A single polarization transmits and receices a single polarization horizontal-horizontal (HH)or vertical-vertical(VV) imager while a dual-polarization system trnamit in one but receive in two HH and HV or VH and VV imagery.(zotero-287?)\n\n\n\n\n\nScattering\nDifferent surface respond differently to the polarizations like bare earth is most sensitive to VV (rough scattering), leaves go cross VH or HV (volume scattering) and trees/buildings sensitive to HH (double bounce).\n\n\n\n\n\n(Earth Science Data Systems 2020)\nA SAR signal has both phase and amplitude data(backscatter). In GEE only amplitude data is available so if we want to use phase data we use SNAP.\nSAR is used in tracking flooding. Area that is flooded appear darker. It is also used for blast damage assessment.\nThis weeks practical takes an example of Beirut explosion. In this case study we are developing our own change detection algorithm using pixel-wise t-test.\nIt is basically just a signal-to-noise ratio. So we have pre and post-event images and it calculates the difference between two sample means(signal) and divides it by the standard deviation of both samples(noise). The t-value is a measure of how many standard deviations the difference between the two mean is. We need to filter the image collection to the ascending and descending orbits, and then calculate the t-value for each orbit separately.(Ballinger 2024)\n\n\n\n\n\n\nThe visualization parameter corresponds the statistical significance of the change in pixel value. dark purple pixel indicate no significant change and yellow pixels indicates a significant change with 95% confidence."
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.2 APPLICATION",
    "text": "8.2 APPLICATION\nSAR data allow us to see through darkness, clouds detecting changes in habitat, level of water or moisture, natural or human disturbance in the earth surface. Therefore taking the example of above practical on battle damage assessment the study discusses the issues of using high resolution optical satellite imagery for getting the highest level of accuracy through Convolution Neural Networks (CNN). As it is financial and computationally expensive plus its not consistent to cloud cover. The advantage of open-access SAR mitigates these problems. It critically analyze (mueller2021?) approach on achieving 0.92 confidence score by training a CNN on damage annotations carried out by the United nations.\nIt analysed the war-related destruction resulting from the Syrian war. Despite the high accuracy, there are number if limitations like cost and cloud. Here we use open-access SAR imagery and pixel-wise T-Test which is a more generalized approach and can be calculated by simply taking the mean and variance of pre and post image over a period of time.(Ballinger 2023)\nIn the similar approach for detection of volcanic ground deformation we can use Sentinel-1 InSAR data. (Albino et al. 2022) paper focuses on 64 volcanic centres understanding ground deformation presenting a framework for real-time volcano monitoring and early warning system. Adiitionally if you generalize it the approach does not work for other regions due to geological differences and the paper has not discussed about validating the result.(Carn 1999)"
  },
  {
    "objectID": "Week9.html#reflection",
    "href": "Week9.html#reflection",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.3 REFLECTION",
    "text": "8.3 REFLECTION\nThis week was mainly on SAR. Its advantages over optical and how it works was very useful to learn. Also the practical we learned this week on Beirut explosion was very interesting as it is using the very simple approach for validation of the model without using any machine learning techniques. As i was very curious to know and how to work on SAR imagery so it really help me understand the perspective and maybe in future I want to do such kind of research.\nAs its the end of term 2 and this is my final entry. I just want to summarize my whole journey as very insightful as I learned so many new tools, read so many policies and got to know different methodologies and satellites.\nI hope who ever reads my diary gets the gist of Remote Sensing and its applications.\nHappy Reading !!\n\n\n\n\nAlbino, Fabien, Juliet Biggs, Milan Lazecký, and Yasser Maghsoudi. 2022. “Routine Processing and Automatic Detection of Volcanic Ground Deformation Using Sentinel-1 InSAR Data: Insights from African Volcanoes.” Remote Sensing 14 (22): 5703. https://doi.org/10.3390/rs14225703.\n\n\nBallinger, Dr Ollie. 2023. “Open Access Damage Detection Using Sentinel-1 Imagery.” https://oballinger.github.io/PWTT/.\n\n\n———. 2024. “CASA0025: Building Spatial Applications with Big Data.” January 10, 2024. https://oballinger.github.io/CASA0025/.\n\n\nCarn, Simon A. 1999. “Application of Synthetic Aperture Radar (SAR) Imagery to Volcano Mapping in the Humid Tropics: A Case Study in East Java, Indonesia.” Bulletin of Volcanology 61 (1): 92–105. https://doi.org/10.1007/s004450050265.\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” Backgrounder. Earth Science Data Systems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar."
  }
]