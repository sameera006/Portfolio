[
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.1 SUMMARY",
    "text": "5.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geo-spatial data sets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (beginner?)\n\n\nThis weeks practical was really interesting as we got to use GEE and all the things we did in previous week can be done in google earth engine too with just few lines of code and without downloading any image.\nWe can make a new point and center the map to it in the code editor.\n\nSo this code chunk will center the point to Delhi. For loading the imagery either Landsat or sentinel-1 or any other we can simply browse it and and load the data. then we can add different layers, create a filter based on image and reduce it using median reducer. Mosaic the images, clip it and calculate different textures like Principal Component Analysis (PCA) and also calculate indices like NDVI.\n\n\n\nPCA Layer\n\n\n\n\n\nNDVI"
  },
  {
    "objectID": "Week6.html#application",
    "href": "Week6.html#application",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.2 APPLICATION",
    "text": "5.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geo spatial analysis software platforms” (Ballinger 2024)\n\n\n\n\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (Xie et al. 2019). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nThere are very clear advantages of GEE with some limitation too as its a web based application.\n\n\n\n\n\n(Amani et al. 2020)\n(Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform. While according to (Hansen et al. 2013), it took 100 h to process 654 178 Landsat-7 images (707 terabytes) within GEE and produce a global map of forests otherwise without this the process would have taken a million hours to complete. Meanwhile we are talking about the advantages of using GEE another paper from (Kumar and Mutanga 2018) has done his research that see in many developing countries, the trend of publishing research output using GEE in most disciplines drops behind that for the developed nations."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "5  Introduction to Google Earth Engine",
    "section": "5.3 REFLECTION",
    "text": "5.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregate function like how to use the reduce function and calculate the mean, median and standard variation of the stack of images.\nThe platform has a learning curve, especially for those who haven't worked with JavaScript before. Additionally, it appears that extra code is necessary to add features like legends, as opposed to the more straightforward button-click approach in QGIS. Another point of confusion is that one must execute the entire script instead of running relevant code chunks individually. Hopefully, future updates will streamline this process.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” January 10, 2024. https://oballinger.github.io/CASA0025/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nHansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, et al. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (6160): 850–53. https://doi.org/10.1126/science.1244693.\n\n\nKumar, Lalit, and Onisimo Mutanga. 2018. “Google Earth Engine Applications Since Inception: Usage, Trends, and Potential.” Remote Sensing 10 (September): 1509. https://doi.org/10.3390/rs10101509.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489.\n\n\nXie, Zunyi, Stuart R. Phinn, Edward T. Game, David J. Pannell, Richard J. Hobbs, Peter R. Briggs, and Eve McDonald-Madden. 2019. “Using Landsat Observations (1988–2017) and Google Earth Engine to Detect Vegetation Cover Changes in Rangelands - A First Step Towards Identifying Degraded Lands for Conservation.” Remote Sensing of Environment 232 (October): 111317. https://doi.org/10.1016/j.rse.2019.111317."
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nCASA0023 Remotely Sensing Cities and Environments is about remotely sensed earth observation(EO) data to make some informed decisions on environmental hazards arising from changing climate.\nWhat is remote sensing?\nAccording to NASA, Remote Sensing is acquiring information from a distance. The data collection may take place directly in the field (insitu or in-place data collection), and/or at some remote distance from the subject matter known as Remote Sensing of the environment.\nExample- the study of daily weather and climate change, land-use/land cover monitoring, food security, military reconnaissance, and many others.\nThe majority of remotely sensed data are analyzed using digital image processing techniques. A very interesting fact about remote sensing image interpretation is that it is both an art and a science.\n\n\n\nFigure 1-Remote Sensing Process (zotero-271?)\n\n\nSpace junk, or space debris, is any piece of machinery or debris left by humans in space. It can refer to big objects such as dead satellites that have failed or been left in orbit at the end of their mission.\n\n\n\nFigure 2 - Space Junk (2017?)\n\n\nSpectral Resolution is the number and dimension (size) of wavelength intervals (referred to as bands or channels) in the electromagnetic spectrum to which a remote sensing instrument is sensitive.\nSpatial Resolution is a measure of the smallest angular or linear separation between two objects that can be resolved by the remote sensing system.\nTemporal Resolution The temporal resolution of a remote sensing system generally refers to how often and when the sensor records imagery of a particular area.\nThe Hot topics being discussed around the world like urban green space and accessibility, Illegal logging, Forest Fire, and temperature studies.\nDifferent types of satellite imagery-\nSentinel 2 -The main advantage of Sentinel 2 over Landsat 8 is its higher resolution (across most bands, it has higher spatial resolution, its revisit time is shorter, and it has more spectral bands). However, Landsat 8 has thermal infrared bands, which means that it can be used to measure temperature.\nLandsat 8 –its history like the origin of satellite imagery USGS -Landsat 8 has a mission to collect global data and give scientists the ability to access changes in Earth’s landscape.\nRemote sensing is performed using an instrument, often referred to as a sensor.\nSensors\nPassive sensors record electromagnetic radiation that is reflected or emitted from the terrain. For example, cameras and video recorders can be used to record visible and near-infrared energy reflected from the terrain.\nand Active sensors such as microwave (RADAR), LiDAR, or SONAR, bathe the terrain in machine-made electromagnetic energy and then record the time-lapsed amount of radiant flux scattered back toward the sensor system.\nRemote sensor data are collected passively (e.g. digital cameras) or actively (e.g., RADAR, LiDAR) using analog or digital remote sensing instruments.\nAtmospheric scattering\n\nAtmospheric scattering causes the colors we see in the sky due to sunlight interacting with particles in the atmosphere.\n\n\n\nRayleigh scattering explains how light interacts with small particles, resulting in blue skies during the day.\n\n\n\n\nFigure 3: Rayleigh Scattering (2024c?)\n\n\nThis week practical we did play with Sentinel and Landsat imagery and compared it with different classes like urban, grass, bare-earth. We compare the spectral signatures from both. we do this by generating point of interest (POIs) matches with both images."
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote sensing plays a crucial role in modern agriculture by providing data-driven insights. It aids in recent yield estimation,facilitates monitoring crop health and development, Detecting infestations remotely.(Shah et al. 2023). Despite advancements in sensor technologies, the agricultural sector faces knowledge gaps regarding the sufficiency, appropriateness, and techno-economic feasibility of remote sensing (RS) technologies.\nAnother aspect of remote sensing can be seen in a phenomena where water used by croplands depends on crop types, soil types, latitude location, type of irrigation, and a host of other issues. So, a proper understanding of these issues need us to inter-link croplands to water use, and food production considering a changing climate and keeping in view environmental sustainability, ecological integrity, and continued robust growth of economy."
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "1  An Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nWhen I think about remote sensing the first thing that comes to my mind is high-resolution images captured by satellites and we humans working on it. Be it land, water, and events like fire, floods, earthquakes, any natural disaster, or man-made disasters like war which includes bombing and destruction of houses. Everything can be recorded, and we can study it, and be aware of it for the future. It’s just like magic. Last week in the Big Data class I was fascinated to see how daily images of Gaza were available and we can see how much destruction has happened or is happening. Remote sensing gives you time-series data through which we can tell how the place has changed in all these years. How many buildings got demolished, how many trees were cut down, and how many new buildings were constructed? In my past life, I have used remotely sensed data to classify land use land-cover and generate environmental impact assessment reports for work. We also studied atmospheric correction which sounds similar to something we did in our GIS class last term. The imagery downloaded can be cleaned just like we clean data or use .csv or .shp files for GIS work. We can remove all the bad layers. Run the software as many times to get the best clean image. It’s like removing all the NaNs. To conclude in more general terms the entire industry is dependent on high-resolution satellite imagery like Google Earth, Google Maps, and Bing Maps.\n\n\n\n\nShah, Adnan Noor, Muhammad Adnan Bukhari, Zahoor Ahmad, Asad Abbas, Abdul Manan, Muhammad Umair Hassan, Muhammad Saqib, et al. 2023. “Application of Remote Sensing in Agriculture.” In, edited by Wajid Nasim Jatoi, Muhammad Mubeen, Muhammad Zaffar Hashmi, Shaukat Ali, Shah Fahad, and Khalid Mahmood, 371–79. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-26692-8_21."
  },
  {
    "objectID": "Week2.html#xaringan-presentation",
    "href": "Week2.html#xaringan-presentation",
    "title": "2  Xaringan",
    "section": "2.1 Xaringan Presentation",
    "text": "2.1 Xaringan Presentation"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "3  Corrections",
    "section": "3.1 SUMMARY",
    "text": "3.1 SUMMARY\nThis week we looked at the story of Virginia Norwood known as the mother of Landsat. She designed the Multispectral Scanner (MSS) against the good old RBV(return beam vidicon) sensor.\nLandsat collects images in long narrow strips called “swaths.” Previous Landsat sensors swept back and forth across the swath like a whisk broom to collect data.\nIn contrast, the instruments on Landsat 8 view across the entire swath at once, building strips of data like a push broom.\n\nhttps://svs.gsfc.nasa.gov/vis/a010000/a012700/a012754/frames/1920x1080_16x9_30p/pushbroomTIFF/\nGEOMETRIC CORRECTION\nGeometric distortions introduced by sensor system attitude (roll, pitch, and yaw) and/or altitude changes can be corrected using ground control points and appropriate mathematical models (e.g., Im et al., 2009). A ground control point (GCP) is a location on the surface of the Earth (e.g., a road intersection) that can be identified on the imagery and located accurately on a map.\nThere are two types of geometric correction-\n\nimage-to-image rectification and\nimage-to-image registration\n\nImage-to-map rectification is the process by which the geometry of an image is made planimetric. The image-to-map rectification process normally involves selecting GCP image pixel coordinates (row and column) with their map coordinates counterparts (e.g., meters northing and easting in a Universal Transverse Mercator map projection).\n\n\n\n\n\nImage-to-image Registration\nImage-to-image registration is the translation and rotation alignment process by which two images of like geometry and of the same geographic area are positioned coincidentally concerning one another so that corresponding elements of the same ground area appear in the same place on the registered images.\n\n\n\nExample of image-to-image hybrid registration\n\n\nATMOSPHERIC CORRECTION\nThe two most important sources of environmental attenuation are\n1) atmosphere attenuation caused by scattering and absorption in the atmosphere, and 2) topographic attenuation.\nFor example, consider the case of the normalized difference vegetation index (NDVI) derived from Landsat Thematic Mapper (TM ) band 3 (red) and band 4 (near-infrared) data.\nA rather simple method to correct raw satellite (or any other imagery) is called dark object subtraction(DOS) which uses the idea that the darkest pixel within the image should be 0 and any value it has is attributed to atmosphere. so to remove it we will subtract that value from the rest of the pixels within the image.\nDIGITAL NUMBER\nFirst, we need to download some raw satellite data that comes in DN format. we are using Lansat 8 collection1 (or 2) level-1 bundle.\nTexture Analysis is one of the most important characteristics dealt with during image interpretation and classification. Texture analysis has been successfully applied to forestry and vegetation studies using a variety of remote sensing data (Asner et al., 2002; Franklin et al., 2000) and radar images (Costa, 2004; Hess et al., 2003)."
  },
  {
    "objectID": "Week3.html#application",
    "href": "Week3.html#application",
    "title": "3  Corrections",
    "section": "3.2 APPLICATION",
    "text": "3.2 APPLICATION\nThe practical content addressed corrections using raw satellite imagery, merging images, and enhancements. This application section will focus on studies that have made use of atmospheric corrections.\nMultispectral and radar satellite remote sensing (SRS) imagery has become an important source for investigating species ecology and ecosystem structure. SRS data fusion techniques, Integrating and fusing multispectral and radar images can significantly improve our ability to assess the distribution as well as the horizontal and vertical structure of ecosystems.\nMultispectral sensors passively measure electromagnetic radiation reflected from the Earth’s surface, radar sensors are active, meaning they emit electromagnetic radiation and then measure the returning signal.\nAdditionally, radar sensors penetrate atmospheric conditions that incapacitate multispectral sensors, such as clouds, haze, and fog, and can (depending on wavelength) return information from below the canopy (Santoro, Shvidenko, McCallum, Askne, & Schmullius, 2007) or even from subsurface layers (McCauley et al., 1982).\nThis type of fusion includes object-level fusion, in which a landscape is divided into multi-pixel objects based on information from different remote sensors (Blaschke, 2010), and pixel-level fusion, where pixel values are combined to derive a fused image with new pixel values, either in the spatial (Zhang, 2010) or the temporal (Reiche, Verbesselt, Hoekman, & Herold, 2015) domain. Since both pixel-and object-level fusions result in a new image, we will here refer to them as image fusion"
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "3  Corrections",
    "section": "3.3 REFLECTION",
    "text": "3.3 REFLECTION\nWell, this week was quite interesting and intense as we learned a lot of different techniques to improve our imagery. What I understood is that when we download imagery from the satellite onto our computer the first step to use it in our analysis is to check its characteristics like the DN, and radiance value. After that, we take measures like atmospheric correction, geometric correction, fusion, and other techniques that improve the quality of imagery. Usually these days the the above corrections are already done so we do not have to check the above but it is a good practice to know all these specifics about the satellite imagery. We can always use image fusion or image enhancement and even change the texture."
  },
  {
    "objectID": "Week4.html#summary",
    "href": "Week4.html#summary",
    "title": "4  Policy",
    "section": "4.1 SUMMARY",
    "text": "4.1 SUMMARY\nClimate-induced desertification is having adverse effects on the African continent each year the Earth continues to warm. It impacts the everyday lives of Africans – from their crops, livestock, and housing – to African wildlife and biodiversity. (Stallwood 2022)\nDesertification is “the process by which fertile land becomes desert, typically as a result of drought, deforestation or inappropriate agriculture.” It is where semi-arid lands, such as grasslands or shrublands, decrease and eventually disappear. (kochquo?)\nAccording to European Commision world atlas of Desertification\n\nOver 75% of the Earth’s land area is already degraded, and over 90% could become degraded by 2050.\nGlobally, a total area half of the size of the European Union (4.18 million km²) is degraded annually, with Africa and Asia being the most affected.\nLand degradation and climate change are estimated to lead to a reduction of global crop yields by about 10% by 2050. Most of this will occur in India, China and sub-Saharan Africa, where land degradation could halve crop production.\nAs a consequence of accelerated deforestation it will become more difficult to mitigate the effects of climate change\nBy 2050, up to 700 million people are estimated to have been displaced due to issues linked to scarce land resources. The figure could reach up to 10 billion by the end of this century.\nWhile land degradation is a global problem, it takes place locally and requires local solutions. Greater commitment and more effective cooperation at the local level are necessary to stop land degradation and loss of biodiversity.\nFurther agricultural expansion, one of the main causes of land degradation, could be limited by increasing yields on existing farmland, shifting to plant-based diets, consuming animal proteins from sustainable sources and reducing food loss and waste.\n\n\n\nFigure 1: Desertification and Climate Change in Africa\n\n\n\nUnder the United Nations’ Sustainable Development Agenda, world leaders have committed to “combat desertification, restore degraded land and soil, including land affected by desertification, drought and floods, and strive to achieve a land degradation-neutral world” by 2030.\nWhile at global level desertification is addressed by the United Nations Convention to Combat Desertification (UNCCD), land degradation is a problem that concerns the United Nations Framework Convention on Combating Climate Change and the Convention on Biodiversity.\nThe importance of land degradation and desertification led to the adoption of Sustainable Development Goal 15.3 aiming at land degradation neutrality."
  },
  {
    "objectID": "Week4.html#application",
    "href": "Week4.html#application",
    "title": "4  Policy",
    "section": "4.2 APPLICATION",
    "text": "4.2 APPLICATION\nThe Sahel region is the most vulnerable region on the continent because of the 1980 drought. The Sahel lies between the Saharan Desert and the Sudanian Savannah. It is a 3,000-mile stretch of land that includes ten counties and is under constant stress due to frequent droughts, soil erosion, and population growth which has increased logging, illegal farming and land clearing for housing. \nAccording to united nation global outlook2 report found that intensive agriculture methods are responsible for upto 80% of deforestation.\nWith the Sahel region being the most vulnerable and heavily affected by desertification, an initiative known as ‘The Green Wall’ was put in place for the Sahara and Sahel in 2007. Its ambitious aim is to grow an 8,000-kilometre natural wonder across the entire width of Africa in order to increase the amount of arable land bordering the Sahara desert. The idea is that planting more trees will combat desertification, create jobs, increase food security and bring migrated populations back home to Africa.\nThe Great Green Wall’s goal for 2030 is to restore 247 million acres of destroyed land and create 10 million jobs in affected rural areas.AFR100’s goal of restoring 100 million hectares by 2030 is not as far-fetched as we may think despite the ambitious goal, especially since the Great Green wall received $14 billion in funding for the next ten years at the recent One Planet Summit for Biodiversity. (Stallwood 2022)\n\n\n\nFigure 2: The Eleven Nations that are investing in the Great Green Wall Initiative _@National Geographic (Jose 2023)\n\n\nThe Great Green Wall concept was first put forth in the early 2000s in response to the Sahel region’s increasing desertification and degradation of the land.\nSome examples of case studies-\nMonitoring methods based on desertificatio indicators mainly include the single indicator method and multiple in dicatormethod. When using a single indicator to evaluate desertifica tion,most studies used vegetation index thresholds to classify the degree of desertification (Bezerra et al., 2020; Filei et al., 2018). Evidently, this method only considers vegetation cover and ignores soil information, leading to the lower accuracy of the classification results (Wei et al., 2018). The multiple indicator method includes two categories: use of the feature space models for classification, and use of machine learning methods to construct evaluation models and classification criteria. Common feature space models include the albedo-normalized differen tialvegetation index (NDVI) model (Wei et al., 2020) and point-point model (Guo et al., 2020). Because such models only consider two in dicators,it is difficult to accurately monitor complex desertificatio information (Duan et al., 2019). In the common multiple indicator classification methods based on machine learning methods, the in dicatorsmainly include the NDVI, albedo, topsoil grain size index (TGSI), and land surface temperature.\nThe spatio temporalcharacteristics of desertification in Mongolia in 1990–2020 and revealed its driving forces. Based on all Landsat images in 1990–2020 in Mongolia, extensive ground surveys, and the existing desertification classification system, we used decision tree, support vector machine (SVM), RF, naive Bayes (NB), minimum distance (MD), and maximum entropy (ME) classifiers, to extract desertification information for seven periods (1990, 1995, 2000, 2005, 2010, 2015, and 2020)."
  },
  {
    "objectID": "Week4.html#reflection",
    "href": "Week4.html#reflection",
    "title": "4  Policy",
    "section": "4.3 REFLECTION",
    "text": "4.3 REFLECTION\n**NEED TO UPDATE it\nIf i am a city planner how can we start to address this What areas should i start with (funding often limited) What stage in the planning process might this come Who would be responsible (e.g. what department) What skills do they need What benefits can this bring to the city (often finances!) How does this help the city align with global agendas What stakeholders do i need to consider\n\n\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green Wall, Africa.” RTF | Rethinking The Future. November 3, 2023. https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\nStallwood, Paige. 2022. “Desertification in Africa: Causes, Effects and Solutions.” Earth.Org. December 15, 2022. https://earth.org/desertification-in-africa/."
  },
  {
    "objectID": "Week5.html#summary",
    "href": "Week5.html#summary",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geospatial datasets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (“Beginner’s Cookbook | Google Earth Engine,” n.d.)"
  },
  {
    "objectID": "Week5.html#application",
    "href": "Week5.html#application",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms” (Ballinger 2024)\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (xie2019?). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nLong et al. (Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform."
  },
  {
    "objectID": "Week5.html#reflection",
    "href": "Week5.html#reflection",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregrate function like hoe to use the reduce function and calculate the mean, median and standard variation of the stack of images.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\n“Beginner’s Cookbook | Google Earth Engine.” n.d. https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "6  Classification-1",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nClassification can be used in many studies like Urban Expansion using Landsat data, Air pollution, Urban green Spaces, monitoring forest like illegal logging, and Forest fires. in all these studies land cover is extracted from the earth observation data.\nSo, the process of classification involves inductive learning which means that by looking at the map we know where is ground or barren land or urban or forest area and accordingly we can classify.\nFor classification we use an expert system.\n\n(whatis?)\nso the expert system consist of the knowledge base and inference engine. what we want to find out how a computer replicate human knowledge. and how expert system is applied to remote sensing.\nMachine learning is defined as the science of computer modeling of learning process. there are number of machine learning data decision tree /regression tree.\nCART (Classification and Regression Tree)\nClassification tree classify data into two or more discrete categories. For example Land cover. Its a tree-like graph with nodes and leaves. Another example we can take is if we want to play badminton which depend on the conditions outside hot or cold, the speed of wind and the weather.\n\n\n\n\n\n\n\n\n\n\n\n\n(decision?)\nRegression trees predict continuous dependent variable like numerical variable for example total population. They subset the data into smaller chunks. When we create decision tree the end leaves might be a mixture of categories meaning they are impure known as Gini impurity. The option with the lowest impurity goes to the top of the tree and becomes the root.\nWhile discussing model prediction it is important to understand prediction errors(bias and variance). so it is very important to understand these errors which will help us build accurate models and avoid the mistake of over fitting and under fittingg. (Singh 2018)\nSo decision trees (DTs) are not good with new or big size data. Higher depth DTs are more prone to over fitting and thus leads to higher variance. This shortcoming of DTs is explored by random forest model.\nRandom Forests\nRandom Forests (RFs) are composed of multiple independent decision trees that are trained independently on a random subset of data.\n\n(Bhatia 2019)\nIn RFs we talk about bootstrapping and out of bag(OOB) error which is basically we do not train all the values and what is left is then tested and validated with the model. the left out variables are called OOB.\nhow do we apply these methods with the satellite imagery?\nImage Classification Techniques - it is the process of assigning land cover classes to pixels. For example classes include water, urban, Forest, agriculture and grassland.\n\nSupervised\nUnsupervised\nObject based Image Classification\nUnsupervised classification usually referred as clustering/k-means. some common clustering algorithm are ISODATA.\nSupervised Classification we use Maximum likelihood and support vector machines(SVM)."
  },
  {
    "objectID": "Week7.html#application",
    "href": "Week7.html#application",
    "title": "6  Classification-1",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“Identifying specific crop types, cropland, and cropping patterns using space-based observations is challenging because different crop types and cropping patterns have similarity spectral signatures. But combining satellite imagery with machine learning methods helps in monitoring the above concern. In this study, Sentinel-2 and Landsat-8 time series data were used with the Decision Tree Classifier (DTC) and Random Forest (RF) methods to map cropland and crop types in Gujranwala’s sub-districts. Optical remote sensing NDVI time series data facilitated cropland mapping, revealing wheat as the predominant crop with 85% coverage, while barley occupied only 0.07% of the total crop area. The analysis focused on areas with consistent agricultural calendars, parcel morphology, and climatic conditions. DTC and RF methods showcased wheat’s superior accuracy.”(Tariq et al. 2023)\n“Remote sensing helps study Urban Green Spaces (UGSs), but less is known about non-tree species, change detection, and health assessment. Most research focuses on large UGSs, neglecting smaller areas like street trees and parks, which are still significant. Urban managers need to identify vegetation species to maintain and protect UGSs from invasive species. Previously, this was costly and hard, relying on field surveys. Now, remote sensing allows for accurate and timely identification of vegetation species in urban areas.”(Shahtahmassebi et al. 2021)\nThe key concerns in the above study is the presence of shadow in high spatial resolution imagery which can reduce the accuracy of UGSs mapping ((Zhang and Qiu 2012); (Merry et al. 2014)). There is also a need to develop methods for extracting informative and intelligent information from Google Street View, for example, species characteristics and the quality of UGSs as might be perceived by users of the spaces.\nA new study suggests creating, testing, and implementing a machine learning approach to assess green space quality based on human perception, using transfer learning from pre-existing models. The findings show that the developed models performed well in six key areas: accuracy, precision, recall, F1-score, Cohen’s Kappa, and Average ROC-AUC. (Rustamov, Rustamov, and Zaki 2023)\n\nUsing remote sensing data, particularly the Normalized Difference Vegetation Index (NDVI), is a common method to evaluate green space quantity on satellite images. (Sun et al. 2021) However, this approach has limitations when assessing green space quality. Remote sensing struggles to detect subtle changes in quality-related variables and provides only a two-dimensional view, which may not accurately represent the green space perceived at eye level. Additionally, while quantity matters, green space quality is more crucial for human health. The complexity of feature extraction in Convolutional Neural Networks (CNN) depends on the number of layers, with fewer layers resulting in weaker learning abilities for complex features.(Ayadi and Lachiri 2022)"
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "6  Classification-1",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nSo, this week was about classification and had a lot of content to cover. I think this is the most important lecture because all the methods taught during the lecture are used very commonly for digital image processing and in future will be used mostly. I understood about decision trees and random forest and how machine learning plays an important role in the field of remote sensing. By week 6 all we did earlier is making sense and helpful in understanding about different applications where we can use this like detecting fires, flood risk, desertification and LULC heat maps.\n\n\n\n\nAyadi, Souha, and Zied Lachiri. 2022. “Deep Neural Network for Visual Emotion Recognition Based on ResNet50 Using Song-Speech Characteristics.” 2022 5th International Conference on Advanced Systems and Emergent Technologies (IC_ASET), March, 363–68. https://doi.org/10.1109/IC_ASET53395.2022.9765898.\n\n\nBhatia, Navnina. 2019. “What Is Out of Bag (OOB) Score in Random Forest?” Medium. June 27, 2019. https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n\n\nMerry, Krista, Jacek Siry, Pete Bettinger, and J. M. Bowker. 2014. “Urban Tree Cover Change in Detroit and Atlanta, USA, 1951–2010.” Cities 41 (December): 123–31. https://doi.org/10.1016/j.cities.2014.06.012.\n\n\nRustamov, Jaloliddin, Zahiriddin Rustamov, and Nazar Zaki. 2023. “Green Space Quality Analysis Using Machine Learning Approaches.” Sustainability 15 (10, 10): 7782. https://doi.org/10.3390/su15107782.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Yue lin, Muye Gan, Ke Wang, Arunima Malik, and George Alan Blackburn. 2021. “Remote Sensing of Urban Green Spaces: A Review.” Urban Forestry & Urban Greening 57 (January): 126946. https://doi.org/10.1016/j.ufug.2020.126946.\n\n\nSingh, Seema. 2018. “Understanding the Bias-Variance Tradeoff.” Medium. October 9, 2018. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229.\n\n\nSun, Yi, Xingzhi Wang, Jiayin Zhu, Liangjian Chen, Yuhang Jia, Jean M. Lawrence, Luo-hua Jiang, Xiaohui Xie, and Jun Wu. 2021. “Using Machine Learning to Examine Street Green Space Types at a High Spatial Resolution: Application in Los Angeles County on Socioeconomic Disparities in Exposure.” The Science of the Total Environment 787 (September): 147653. https://doi.org/10.1016/j.scitotenv.2021.147653.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and Faisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and Crop Types by Combining Optical Remote Sensing Images with Decision Tree Classifier and Random Forest.” Geo-Spatial Information Science 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287.\n\n\nZhang, C., and F. Qiu. 2012. “Mapping Individual Tree Species in an Urban Forest Using Airborne Lidar Data and Hyperspectral Imagery: AAG Remote Sensing Specialty Group 2011 Award Winner.” Photogrammetric Engineering and Remote Sensing 78 (10): 1079–87. https://doi.org/10.14358/PERS.78.10.1079."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achanta, Radhakrishna, and Sabine Susstrunk. 2017. “Superpixels\nand Polygons Using Simple Non-iterative\nClustering.” In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition\n(CVPR), 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing\nPlatform for Remote Sensing Big Data Applications:\nA Comprehensive Review.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nAyadi, Souha, and Zied Lachiri. 2022. “Deep Neural\nNetwork for Visual Emotion Recognition Based on\nResNet50 Using Song-Speech\nCharacteristics.” 2022 5th International Conference on\nAdvanced Systems and Emergent Technologies (IC_ASET), March,\n363–68. https://doi.org/10.1109/IC_ASET53395.2022.9765898.\n\n\nBallinger, Dr Ollie. 2023. “Open Access Damage Detection Using\nSentinel-1 Imagery.” https://oballinger.github.io/PWTT/.\n\n\n———. 2024. “CASA0025: Building Spatial\nApplications with Big Data.” January 10,\n2024. https://oballinger.github.io/CASA0025/.\n\n\nBhatia, Navnina. 2019. “What Is Out of\nBag (OOB) Score in Random\nForest?” Medium. June 27, 2019. https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of\nClassifications of Remotely Sensed Data.” Remote Sensing of\nEnvironment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic\nAperture Radar? | Earthdata.” Backgrounder.\nEarth Science Data Systems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nGISGeography. 2016. “Supervised and Unsupervised\nClassification in Remote Sensing.” GIS\nGeography. June 30, 2016. https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-scale Geospatial Analysis for\nEveryone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools, applications and experiences,\n202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green\nWall, Africa.” RTF | Rethinking The Future.\nNovember 3, 2023. https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022.\n“Spatial Dependence Between Training and Test Sets: Another\nPitfall of Classification Accuracy Assessment in Remote Sensing.”\nMachine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang,\nBingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin.\n2019. “30 m Resolution Global Annual Burned Area Mapping\nBased on Landsat Images and Google Earth\nEngine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489.\n\n\nMerry, Krista, Jacek Siry, Pete Bettinger, and J. M. Bowker. 2014.\n“Urban Tree Cover Change in Detroit and\nAtlanta, USA, 1951–2010.”\nCities 41 (December): 123–31. https://doi.org/10.1016/j.cities.2014.06.012.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith,\nGurutzeta Guillera‐Arroita, Severin Hauenstein, et al. 2017.\n“Cross‐validation Strategies for Data with Temporal, Spatial,\nHierarchical, or Phylogenetic Structure.” Ecography 40\n(8): 913–29. https://doi.org/10.1111/ecog.02881.\n\n\nRustamov, Jaloliddin, Zahiriddin Rustamov, and Nazar Zaki. 2023.\n“Green Space Quality Analysis Using Machine Learning\nApproaches.” Sustainability 15 (10, 10): 7782. https://doi.org/10.3390/su15107782.\n\n\nShah, Adnan Noor, Muhammad Adnan Bukhari, Zahoor Ahmad, Asad Abbas,\nAbdul Manan, Muhammad Umair Hassan, Muhammad Saqib, et al. 2023.\n“Application of Remote Sensing in Agriculture.” In, edited\nby Wajid Nasim Jatoi, Muhammad Mubeen, Muhammad Zaffar Hashmi, Shaukat\nAli, Shah Fahad, and Khalid Mahmood, 371–79. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-031-26692-8_21.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Yue lin, Muye\nGan, Ke Wang, Arunima Malik, and George Alan Blackburn. 2021.\n“Remote Sensing of Urban Green Spaces: A\nReview.” Urban Forestry & Urban Greening 57\n(January): 126946. https://doi.org/10.1016/j.ufug.2020.126946.\n\n\nSingh, Seema. 2018. “Understanding the Bias-Variance\nTradeoff.” Medium. October 9, 2018. https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229.\n\n\nStallwood, Paige. 2022. “Desertification in Africa:\nCauses, Effects and\nSolutions.” Earth.Org. December 15, 2022. https://earth.org/desertification-in-africa/.\n\n\nSun, Yi, Xingzhi Wang, Jiayin Zhu, Liangjian Chen, Yuhang Jia, Jean M.\nLawrence, Luo-hua Jiang, Xiaohui Xie, and Jun Wu. 2021. “Using\nMachine Learning to Examine Street Green Space Types at a High Spatial\nResolution: Application in Los Angeles County\non Socioeconomic Disparities in Exposure.” The Science of the\nTotal Environment 787 (September): 147653. https://doi.org/10.1016/j.scitotenv.2021.147653.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and\nFaisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and\nCrop Types by Combining Optical Remote Sensing Images with Decision Tree\nClassifier and Random Forest.” Geo-Spatial Information\nScience 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287.\n\n\nXie, Zunyi, Stuart R. Phinn, Edward T. Game, David J. Pannell, Richard\nJ. Hobbs, Peter R. Briggs, and Eve McDonald-Madden. 2019. “Using\nLandsat Observations (1988–2017) and Google Earth\nEngine to Detect Vegetation Cover Changes in Rangelands -\nA First Step Towards Identifying Degraded Lands for\nConservation.” Remote Sensing of Environment 232\n(October): 111317. https://doi.org/10.1016/j.rse.2019.111317.\n\n\nZhang, C., and F. Qiu. 2012. “Mapping Individual Tree Species in\nan Urban Forest Using Airborne Lidar Data and Hyperspectral Imagery:\nAAG Remote Sensing Specialty Group 2011 Award\nWinner.” Photogrammetric Engineering and Remote Sensing\n78 (10): 1079–87. https://doi.org/10.14358/PERS.78.10.1079."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Learning Diary",
    "section": "",
    "text": "ABOUT\nThis is my online Learning diary.\nIn CASA0023 we are asked to make a portfolio that will help us in the future.\nHi! I am Sameera. I am from India and have lived in many different parts of the country. When someone ask me where in India you are from i have to think and reply as i was born in Kanpur, completed my school there and for further studies I moved to Delhi, the capital of India. and Past five years i have been moving to different cities from Ahmedabad to Kochi to Bangalore. Now here i am in London.\nReading maps has fascinated me ever since I attended cartography lessons back in school; however, I never knew that I would find my calling in the same field in the future. It is exhilarating how I followed my passion and entered the field of GIS after spending over six years working as a lecturer at an undergraduate college.\nI love travelling. My favorite destination till date is Leh, Ladakh. Here’s the picture from that place."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "7  Classification- The Big Question",
    "section": "7.1 SUMMARY",
    "text": "7.1 SUMMARY\nThere is some Pre-classified data which can be used where we don’t need to do the classification. For example Dynamic World Data which use SR for labelling but use TOA for the model. here we use Machine learning models like Convolution Neural Network. Accuracy is assessed through Confusion Matrix.\nWhen we use such classified data we check that how was it trained. There are some issues like blobbiness as its a 50x50 m image.\nOBJECT BASED IMAGE ANALYSIS (OBIA)\nHere we talk about pixels and distance mainly Euclidian distance. OBIA is similarity between pixels. It tries to match the similar pixels. for example - Take a RGB near-infrared 1m image. Its a typical kind of image where we want to do segmentation and OBIA. we do OBIA to do clean classification. Like if we classify on the mean of all the vectors inside a particular polygon we will get a solid result as opposed to classifying on individual pixels we will get a lot salt and pepper noise.\nThere comes also a question why do we need object based analysis? It is to improve in signal and noise. There are thousand pixels here so we might want to turn these things into objects.\n\n\n\n\n\n\n\n\nOriginal Image to Classify\nDifferent Crop Types\nPer Pixel Classification\n\n\n\n\n\n\n\n\n\n\nFig1: Process of Classification\nWe use mode reducer in neighborhood to clean the image. it will remove the single pixel outliers. we also talk about spectral gradient and can we use super-pixels to remove the salt and pepper noise.\n\nFig2: Spectral Gradient and Distances\nSNIC- (Super pixel non-iterative clustering) Seeded region growing\nThis makes clusters without using k-means. It uses a regular grid of points (like k-means) but then assigns pixels to points through distance color and co-ordinates - it represents normalised spatial and color distances. we also have seed grid which denotes spacing in pixel. we can set square or hex grid.\nImage.reduceConnectedComponents\n(Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the most common method regular points on the image work out spatial distance (from point to centre of pixel) = closeness to centre colour difference (RGB vs RGB to centre point) = homogenity of colours\nThe SLIC algorithm works iteratively, repeating the above process until it reaches the expected number of iterations.\nsub pixel analysis/ spectral mixture analysis\nSupervised Classification in Remote Sensing\nIn supervised classification, you select training samples and classify your image based on your chosen samples. Your training samples are key because they will determine which class each pixel inherits in your overall image.\n\nFig3: Classification and Validation(GISGeography 2016)\nThe good approach is to train and test split\n\nThis is simply holding back a % of the original data used to train the model to then test it at the end\n\n\nFig.4: Testing and Training the ML model\nWhat do you think of spatial autocorrelation in this sense? Is OBIA is dealing with spatial autocorrelation?\nIn ML model if we use spatial data and they have not considered spatial autocorrelation between its training and testing data then its considered too good or overfitted. To solve this we do spatial cross validation"
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "7  Classification- The Big Question",
    "section": "7.2 APPLICATION",
    "text": "7.2 APPLICATION\nThe researchers lately have diverted themselves to a much simpler task of simplifying an image into a smaller cluster of pixels called super pixels than using traditional segmentation algorithms. Several applications such as object localization, multi-class segmentation, optical flow, body model estimation, object tracking, depth estimation took advantage of super pixels.\nThe algorithm to do this is Simple linear Iterative Clustering Algorithm(SLIC) but it has some limitations. It requires several iterations for the centroids to converge. It uses a distance map of the same size as the number of input pixels, which amounts to significant memory consumption for image stacks or video volumes. Lastly, SLIC enforces connectivity only as a post-processing step. So the better version of this is simple non-iterative clustering (SNIC). It is non-iterative, explicitly enforces connectivity, is computationally cheaper, uses lesser memory.(Achanta and Susstrunk 2017)\nSpatial autocorrelation is inherent to remotely sensed data. nearby pixels are more similar than distant ones. The evaluation of classification accuracy has always been considered as an important issue in the remote sensing community. (Congalton 1991)\nDespite the importance attached to the accuracy assessment protocol, a gap, sometimes a serious one, is often found between the performance metrics of a model and the real quality of the resulting map. This tends to reduce the confidence placed in accuracy statistics and to discredit the true capacity of remote sensing in the opinion of the t end users. Because of spatial autocorrelation, spectral values of close pixels are often more similar than those of distant ones, producing falsely high accuracy metrics if the sampling design is not used for testing. (Roberts et al. 2017)\nFrom a statistical point of view, two problems can arise with spatial autocorrelation: (1) spatial non-independence of the classification errors (or model residuals) and (2) spatial non-independence of the training and test sets used for accuracy assessment.\nThe six cross-validation strategies with different sample sizes were applied to the department of Herault-34. The classification performances obtained by cross-validation on Herault-34 are given in Fig. 5. Overall, the results show that ignoring dependence between training and test sets leads to very high accuracy metrics whatever the sample size. This is particularly clear at the pixel level but also at the object level with large samples. Compared to sampling strategies that account for spatial autocorrelation, the accuracy metrics are overestimated. Our results revealed notable underestimation of generalization errors when traditional non spatial approaches were used to assess the accuracy. Pixel-based samplings were the most affected. Object-based strategies mitigate the effect of spatial dependence since the pixels used for training and testing never belong to the same forest stands. Nonetheless, non-spatial data-splitting at the object level also leads to overestimation of predictive performance. we need to change practices in classification accuracy assessment using spatial imagery. A data splitting design ensuring spatial independence between the training and test sets should be the standard approach for validation. (Karasiak et al. 2022)\n\nFig.5: Average Overall accuracy based on the RF classifier"
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "7  Classification- The Big Question",
    "section": "7.3 REFLECTION",
    "text": "7.3 REFLECTION\nThis week we discussed about object based image analysis i think is very important part of classification. The issues related to that and if for example we have spatial data then we always have spatial autocorrelation. They are kind of related . So we need to keep a check on that which can be resolved through spatial cross validation or object based image analysis. If we are using machine learning models for spatial data we need to take care of overfitting of data and how can we make the more generalized model which can be use for different imagery. this week also talks about lots of methods we discussed in GIS module like Moran’s I, K-means Clustering and GWR. So here we see the merge of Remote Sensing and GIS.\n\n\n\n\nAchanta, Radhakrishna, and Sabine Susstrunk. 2017. “Superpixels and Polygons Using Simple Non-iterative Clustering.” In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data.” Remote Sensing of Environment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nGISGeography. 2016. “Supervised and Unsupervised Classification in Remote Sensing.” GIS Geography. June 30, 2016. https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera‐Arroita, Severin Hauenstein, et al. 2017. “Cross‐validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881."
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.1 SUMMARY",
    "text": "8.1 SUMMARY\nSynthetic Aperture Radar (SAR) data\n“SAR instruments send pulses of microwaves toward Earth’s surface and listen for the reflections of those waves. The radar waves can penetrate cloud cover, vegetation, and the dark of night to detect changes that might not show up in visible light imagery. When Earth’s crust moves due to an earthquake, when dry land is suddenly covered by flood water, or when buildings have been damaged or toppled, the amplitude and phase of radar wave reflections changes in those areas and indicates to the satellite that something on the ground has changed.”\nIt is an active sensor which means a sensor which provides their own illumination. For example it sees the world as the way bats do as they emit a chirp and listen to the backscatter of an echo if something gets reflected while passive sensor (optical) sees the world as our eyes or camera does.\nRadar sensor picks up the signal reflected backscattered from the earth surface. They check the topology of the surface based on how it reflects.\nInterpreting Radar Images\n\nRegions of calm water and other smooth surfaces appear black, because the radar pulse reflects away from the spacecraft\nRough surfaces appear brighter, as they reflect the radar in all directions, and more of the energy is scattered back to the antenna. \n\nDifferent wavelengths penetrates through different surfaces used in different applications. So choice of wavelenghth is important.\n\n\n\n\n\n(Earth Science Data Systems 2020)\nThe most commonly used Band in imaging is C-band with 4-8 GHz frequency.\nPolarization\nImaging radars have different polarization configurations. A single polarization transmits and receices a single polarization horizontal-horizontal (HH)or vertical-vertical(VV) imager while a dual-polarization system trnamit in one but receive in two HH and HV or VH and VV imagery.(zotero-287?)\n\n\n\n\n\nScattering\nDifferent surface respond differently to the polarizations like bare earth is most sensitive to VV (rough scattering), leaves go cross VH or HV (volume scattering) and trees/buildings sensitive to HH (double bounce).\n\n\n\n\n\n(Earth Science Data Systems 2020)\nA SAR signal has both phase and amplitude data(backscatter). In GEE only amplitude data is available so if we want to use phase data we use SNAP.\nSAR is used in tracking flooding. Area that is flooded appear darker. It is also used for blast damage assessment.\nThis weeks practical takes an example of Beirut explosion. In this case study we are developing our own change detection algorithm using pixel-wise t-test.\nIt is basically just a signal-to-noise ratio. So we have pre and post-event images and it calculates the difference between two sample means(signal) and divides it by the standard deviation of both samples(noise). The t-value is a measure of how many standard deviations the difference between the two mean is. We need to filter the image collection to the ascending and descending orbits, and then calculate the t-value for each orbit separately.(Ballinger 2024)\n\n\n\n\n\n\nThe visualization parameter corresponds the statistical significance of the change in pixel value. dark purple pixel indicate no significant change and yellow pixels indicates a significant change with 95% confidence."
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.2 APPLICATION",
    "text": "8.2 APPLICATION\nSAR data allow us to see through darkness, clouds detecting changes in habitat, level of water or moisture, natural or human disturbance in the earth surface. Therefore taking the example of above practical on battle damage assessment the study discusses the issues of using high resolution optical satellite imagery for getting the highest level of accuracy through Convolution Neural Networks (CNN). As it is financial and computationally expensive plus its not consistent to cloud cover. The advantage of open-access SAR mitigates these problems. It critically analyze (mueller2021?) approach on achieving 0.92 confidence score by training a CNN on damage annotations carried out by the United nations.\nIt analysed the war-related destruction resulting from the Syrian war. Despite the high accuracy, there are number if limitations like cost and cloud. Here we use open-access SAR imagery and pixel-wise T-Test which is a more generalized approach and can be calculated by simply taking the mean and variance of pre and post image over a period of time.(Ballinger 2023)\nIn the similar approach for detection of volcanic ground deformation we can use Sentinel-1 InSAR data. (Albino et al. 2022) paper focuses on 64 volcanic centres understanding ground deformation presenting a framework for real-time volcano monitoring and early warning system. Adiitionally if you generalize it the approach does not work for other regions due to geological differences and the paper has not discussed about validating the result.(Carn 1999)"
  },
  {
    "objectID": "Week9.html#reflection",
    "href": "Week9.html#reflection",
    "title": "8  Synthetic Aperture Radar (SAR) Data",
    "section": "8.3 REFLECTION",
    "text": "8.3 REFLECTION\nThis week was mainly on SAR. Its advantages over optical and how it works was very useful to learn. Also the practical we learned this week on Beirut explosion was very interesting as it is using the very simple approach for validation of the model without using any machine learning techniques. As i was very curious to know and work on SAR imagery so it really help me understand the perspective and maybe in future I want to do such kind of research. As its the end of term 2 and this is my final entry. I just want to summarize my whole journey as very insightful as I learned so many new tools, read so many policies and got to know different methodologies and satellites.\nI hope who ever reads my diary gets the gist of Remote Sensing and its applications.\nHappy Reading !!\n\n\n\n\nAlbino, Fabien, Juliet Biggs, Milan Lazecký, and Yasser Maghsoudi. 2022. “Routine Processing and Automatic Detection of Volcanic Ground Deformation Using Sentinel-1 InSAR Data: Insights from African Volcanoes.” Remote Sensing 14 (22): 5703. https://doi.org/10.3390/rs14225703.\n\n\nBallinger, Dr Ollie. 2023. “Open Access Damage Detection Using Sentinel-1 Imagery.” https://oballinger.github.io/PWTT/.\n\n\n———. 2024. “CASA0025: Building Spatial Applications with Big Data.” January 10, 2024. https://oballinger.github.io/CASA0025/.\n\n\nCarn, Simon A. 1999. “Application of Synthetic Aperture Radar (SAR) Imagery to Volcano Mapping in the Humid Tropics: A Case Study in East Java, Indonesia.” Bulletin of Volcanology 61 (1): 92–105. https://doi.org/10.1007/s004450050265.\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” Backgrounder. Earth Science Data Systems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar."
  }
]