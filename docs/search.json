[
  {
    "objectID": "Week6.html#summary",
    "href": "Week6.html#summary",
    "title": "6  Week 6 - Introduction to Google Earth Engine",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geospatial datasets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (“Beginner’s Cookbook | Google Earth Engine,” n.d.)"
  },
  {
    "objectID": "Week6.html#application",
    "href": "Week6.html#application",
    "title": "6  Week 6 - Introduction to Google Earth Engine",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms” (Ballinger 2024)\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (xie2019?). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nLong et al. (Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform."
  },
  {
    "objectID": "Week6.html#reflection",
    "href": "Week6.html#reflection",
    "title": "6  Week 6 - Introduction to Google Earth Engine",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregrate function like hoe to use the reduce function and calculate the mean, median and standard variation of the stack of images.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\n“Beginner’s Cookbook | Google Earth Engine.” n.d. https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489."
  },
  {
    "objectID": "Week1.html#summary",
    "href": "Week1.html#summary",
    "title": "2  Week 1- An Introduction to Remote Sensing",
    "section": "2.1 Summary",
    "text": "2.1 Summary\nCASA0023 Remotely Sensing Cities and Environments is about remotely sensed earth observation(EO) data to make some informed decisions on environmental hazards arising from changing climate.\nWhat is remote sensing?\nAccording to NASA, Remote Sensing is acquiring information from a distance. The data collection may take place directly in the field (in situ or in-place data collection), and/or at some remote distance from the subject matter known as Remote Sensing of the environment. Example- the study of daily weather and climate change, land-use/land cover monitoring, food security, military reconnaissance, and many others.\nThe majority of remotely sensed data are analyzed using digital image processing techniques. A very interesting fact about remote sensing image interpretation is that it is both an art and a science.\n\n\n\n\n\nSpace junk, or space debris, is any piece of machinery or debris left by humans in space. It can refer to big objects such as dead satellites that have failed or been left in orbit at the end of their mission. It can also refer to smaller things, like bits of debris or paint flecks that have fallen off a rocket. Some human-made junk has been left on the Moon, too.\n\n\n\n\n\nSpectral Resolution is the number and dimension (size) of wavelength intervals (referred to as bands or channels) in the electromagnetic spectrum to which a remote sensing instrument is sensitive.\nSpatial Resolution is a measure of the smallest angular or linear separation between two objects that can be resolved by the remote sensing system.\nTemporal Resolution The temporal resolution of a remote sensing system generally refers to how often and when the sensor records imagery of a particular area.\nThe Hot topics being discussed around the world like urban green space and accessibility, Illegal logging, Forest Fire, and temperature studies.\nDifferent types of satellite imagery.\nSentinel 2\nThe main advantage of Sentinel 2 over Landsat 8 is its higher resolution (across most bands, it has higher spatial resolution, its revisit time is shorter, and it has more spectral bands). However, Landsat 8 has thermal infrared bands, which means that it can be used to measure temperature.\nLandsat 8 –its history like the origin of satellite imagery USGS -Landsat 8 has a mission to collect global data and give scientists the ability to access changes in Earth’s landscape.\nRemote sensing is performed using an instrument, often referred to as a sensor.\nTwo types of Sensors\nPassive sensors record electromagnetic radiation that is reflected or emitted from the terrain. For example, cameras and video recorders can be used to record visible and near-infrared energy reflected from the terrain.\nand Active such as microwave (RADAR), LiDAR, or SONAR, bathe the terrain in machine-made electromagnetic energy and then record the time-lapsed amount of radiant flux scattered back toward the sensor system.\nRemote sensor data are collected passively (e.g. digital cameras) or actively (e.g., RADAR, LiDAR) using analog or digital remote sensing instruments.\nAtmospheric scattering\nAtmospheric scattering is the natural phenomenon mainly responsible for the colours we observe in the sky\nAtmospheric scattering is a natural phenomenon that can be described as the result of the interaction between sunlight and particles in the atmosphere. When we look to the sky during the day we see mostly blue, while at sunset we get a more reddish color, especially near the horizon.\nRayleigh scattering describes the behavior of light when interacting with very small particles (most of the particles present in the atmosphere). Mie scattering can be used to describe the behavior of light interacting with any kind of particles, nevertheless, it is mostly used to describe the interaction with larger particles such as haze. The core of the atmospheric scattering computational models is the method to solve the scattering equations of Mie and Rayleigh."
  },
  {
    "objectID": "Week1.html#application",
    "href": "Week1.html#application",
    "title": "2  Week 1- An Introduction to Remote Sensing",
    "section": "2.2 Application",
    "text": "2.2 Application"
  },
  {
    "objectID": "Week1.html#reflection",
    "href": "Week1.html#reflection",
    "title": "2  Week 1- An Introduction to Remote Sensing",
    "section": "2.3 Reflection",
    "text": "2.3 Reflection\nWhen I think about remote sensing the first thing that comes to my mind is high-resolution images captured by satellites and we humans working on it. Be it land, water, and events like fire, floods, earthquakes, any natural disaster, or man-made disasters like war which includes bombing and destruction of houses. Everything can be recorded, and we can study it, and be aware of it for the future. It’s just like magic. Last week in the Big Data class I was fascinated to see how daily images of Gaza were available and we can see how much destruction has happened or is happening. Remote sensing gives you time-series data through which we can tell how the place has changed in all these years. How many buildings got demolished, how many trees were cut down, and how many new buildings were constructed? In my past life, I have used remotely sensed data to classify land use land-cover and generate environmental impact assessment reports for work. We also studied atmospheric correction which sounds similar to something we did in our GIS class last term. The imagery downloaded can be cleaned just like we clean data or use .csv or .shp files for GIS work. We can remove all the bad layers. Run the software as many times to get the best clean image. It’s like removing all the NaNs. To conclude in more general terms the entire industry is dependent on high-resolution satellite imagery like Google Earth, Google Maps, and Bing Maps. (earthsciencedatasystems2023?)"
  },
  {
    "objectID": "Week2.html#xaringan-presentation",
    "href": "Week2.html#xaringan-presentation",
    "title": "3  Week2- Presentation",
    "section": "3.1 Xaringan Presentation",
    "text": "3.1 Xaringan Presentation"
  },
  {
    "objectID": "Week3.html#summary",
    "href": "Week3.html#summary",
    "title": "4  Week 3-Corrections",
    "section": "4.1 SUMMARY",
    "text": "4.1 SUMMARY\nThis week we looked at the story of Virginia Norwood known as the mother of Landsat. She designed the Multispectral Scanner (MSS) against the good old RBV(return beam vidicon) sensor.\nLandsat collects images in long narrow strips called “swaths.” Previous Landsat sensors swept back and forth across the swath like a whisk broom to collect data.\nIn contrast, the instruments on Landsat 8 view across the entire swath at once, building strips of data like a push broom.\n\nhttps://svs.gsfc.nasa.gov/vis/a010000/a012700/a012754/frames/1920x1080_16x9_30p/pushbroomTIFF/\nGEOMETRIC CORRECTION\nGeometric distortions introduced by sensor system attitude (roll, pitch, and yaw) and/or altitude changes can be corrected using ground control points and appropriate mathematical models (e.g., Im et al., 2009). A ground control point (GCP) is a location on the surface of the Earth (e.g., a road intersection) that can be identified on the imagery and located accurately on a map.\nThere are two types of geometric correction-\n\nimage-to-image rectification and\nimage-to-image registration\n\nImage-to-map rectification is the process by which the geometry of an image is made planimetric. The image-to-map rectification process normally involves selecting GCP image pixel coordinates (row and column) with their map coordinates counterparts (e.g., meters northing and easting in a Universal Transverse Mercator map projection).\n\n\n\n\n\nImage-to-image Registration\nImage-to-image registration is the translation and rotation alignment process by which two images of like geometry and of the same geographic area are positioned coincidentally concerning one another so that corresponding elements of the same ground area appear in the same place on the registered images.\n\n\n\nExample of image-to-image hybrid registration\n\n\nATMOSPHERIC CORRECTION\nThe two most important sources of environmental attenuation are\n1) atmosphere attenuation caused by scattering and absorption in the atmosphere, and 2) topographic attenuation.\nFor example, consider the case of the normalized difference vegetation index (NDVI) derived from Landsat Thematic Mapper (TM ) band 3 (red) and band 4 (near-infrared) data.\nA rather simple method to correct raw satellite (or any other imagery) is called dark object subtraction(DOS) which uses the idea that the darkest pixel within the image should be 0 and any value it has is attributed to atmosphere. so to remove it we will subtract that value from the rest of the pixels within the image.\nDIGITAL NUMBER\nFirst, we need to download some raw satellite data that comes in DN format. we are using Lansat 8 collection1 (or 2) level-1 bundle.\nTexture Analysis is one of the most important characteristics dealt with during image interpretation and classification. Texture analysis has been successfully applied to forestry and vegetation studies using a variety of remote sensing data (Asner et al., 2002; Franklin et al., 2000) and radar images (Costa, 2004; Hess et al., 2003)."
  },
  {
    "objectID": "Week3.html#application",
    "href": "Week3.html#application",
    "title": "4  Week 3-Corrections",
    "section": "4.2 APPLICATION",
    "text": "4.2 APPLICATION\nThe practical content addressed corrections using raw satellite imagery, merging images, and enhancements. This application section will focus on studies that have made use of atmospheric corrections.\nMultispectral and radar satellite remote sensing (SRS) imagery has become an important source for investigating species ecology and ecosystem structure. SRS data fusion techniques, Integrating and fusing multispectral and radar images can significantly improve our ability to assess the distribution as well as the horizontal and vertical structure of ecosystems.\nMultispectral sensors passively measure electromagnetic radiation reflected from the Earth’s surface, radar sensors are active, meaning they emit electromagnetic radiation and then measure the returning signal.\nAdditionally, radar sensors penetrate atmospheric conditions that incapacitate multispectral sensors, such as clouds, haze, and fog, and can (depending on wavelength) return information from below the canopy (Santoro, Shvidenko, McCallum, Askne, & Schmullius, 2007) or even from subsurface layers (McCauley et al., 1982).\nThis type of fusion includes object-level fusion, in which a landscape is divided into multi-pixel objects based on information from different remote sensors (Blaschke, 2010), and pixel-level fusion, where pixel values are combined to derive a fused image with new pixel values, either in the spatial (Zhang, 2010) or the temporal (Reiche, Verbesselt, Hoekman, & Herold, 2015) domain. Since both pixel-and object-level fusions result in a new image, we will here refer to them as image fusion"
  },
  {
    "objectID": "Week3.html#reflection",
    "href": "Week3.html#reflection",
    "title": "4  Week 3-Corrections",
    "section": "4.3 REFLECTION",
    "text": "4.3 REFLECTION\nWell, this week was quite interesting and intense as we learned a lot of different techniques to improve our imagery. What I understood is that when we download imagery from the satellite onto our computer the first step to use it in our analysis is to check its characteristics like the DN, and radiance value. After that, we take measures like atmospheric correction, geometric correction, fusion, and other techniques that improve the quality of imagery. Usually these days the the above corrections are already done so we do not have to check the above but it is a good practice to know all these specifics about the satellite imagery. We can always use image fusion or image enhancement and even change the texture."
  },
  {
    "objectID": "Week4.html#summary",
    "href": "Week4.html#summary",
    "title": "5  Week 4 - Policy",
    "section": "5.1 SUMMARY",
    "text": "5.1 SUMMARY\nClimate-induced desertification is having adverse effects on the African continent each year the Earth continues to warm. It impacts the everyday lives of Africans – from their crops, livestock, and housing – to African wildlife and biodiversity. (Stallwood 2022)\nDesertification is “the process by which fertile land becomes desert, typically as a result of drought, deforestation or inappropriate agriculture.” It is where semi-arid lands, such as grasslands or shrublands, decrease and eventually disappear. (“Koch Quoted in BBC Article on Dubai, Desertification,” n.d.)\nAccording to European Commision world atlas of Desertification\n\nOver 75% of the Earth’s land area is already degraded, and over 90% could become degraded by 2050.\nGlobally, a total area half of the size of the European Union (4.18 million km²) is degraded annually, with Africa and Asia being the most affected.\nLand degradation and climate change are estimated to lead to a reduction of global crop yields by about 10% by 2050. Most of this will occur in India, China and sub-Saharan Africa, where land degradation could halve crop production.\nAs a consequence of accelerated deforestation it will become more difficult to mitigate the effects of climate change\nBy 2050, up to 700 million people are estimated to have been displaced due to issues linked to scarce land resources. The figure could reach up to 10 billion by the end of this century.\nWhile land degradation is a global problem, it takes place locally and requires local solutions. Greater commitment and more effective cooperation at the local level are necessary to stop land degradation and loss of biodiversity.\nFurther agricultural expansion, one of the main causes of land degradation, could be limited by increasing yields on existing farmland, shifting to plant-based diets, consuming animal proteins from sustainable sources and reducing food loss and waste.\n\n\n\nFigure 1: Desertification and Climate Change in Africa\n\n\n\nUnder the United Nations’ Sustainable Development Agenda, world leaders have committed to “combat desertification, restore degraded land and soil, including land affected by desertification, drought and floods, and strive to achieve a land degradation-neutral world” by 2030.\nWhile at global level desertification is addressed by the United Nations Convention to Combat Desertification (UNCCD), land degradation is a problem that concerns the United Nations Framework Convention on Combating Climate Change and the Convention on Biodiversity.\nThe importance of land degradation and desertification led to the adoption of Sustainable Development Goal 15.3 aiming at land degradation neutrality."
  },
  {
    "objectID": "Week4.html#application",
    "href": "Week4.html#application",
    "title": "5  Week 4 - Policy",
    "section": "5.2 APPLICATION",
    "text": "5.2 APPLICATION\nThe Sahel region is the most vulnerable region on the continent because of the 1980 drought. The Sahel lies between the Saharan Desert and the Sudanian Savannah. It is a 3,000-mile stretch of land that includes ten counties and is under constant stress due to frequent droughts, soil erosion, and population growth which has increased logging, illegal farming and land clearing for housing. \nAccording to united nation global outlook2 report found that intensive agriculture methods are responsible for upto 80% of deforestation.\nWith the Sahel region being the most vulnerable and heavily affected by desertification, an initiative known as ‘The Green Wall’ was put in place for the Sahara and Sahel in 2007. Its ambitious aim is to grow an 8,000-kilometre natural wonder across the entire width of Africa in order to increase the amount of arable land bordering the Sahara desert. The idea is that planting more trees will combat desertification, create jobs, increase food security and bring migrated populations back home to Africa.\nThe Great Green Wall’s goal for 2030 is to restore 247 million acres of destroyed land and create 10 million jobs in affected rural areas.AFR100’s goal of restoring 100 million hectares by 2030 is not as far-fetched as we may think despite the ambitious goal, especially since the Great Green wall received $14 billion in funding for the next ten years at the recent One Planet Summit for Biodiversity. (Stallwood 2022)\n\n\n\nFigure 2: The Eleven Nations that are investing in the Great Green Wall Initiative _@National Geographic (Jose 2023)\n\n\nThe Great Green Wall concept was first put forth in the early 2000s in response to the Sahel region’s increasing desertification and degradation of the land.\nSome examples of case studies-\nMonitoring methods based on desertificatio indicators mainly include the single indicator method and multiple in dicatormethod. When using a single indicator to evaluate desertifica tion,most studies used vegetation index thresholds to classify the degree of desertification (Bezerra et al., 2020; Filei et al., 2018). Evidently, this method only considers vegetation cover and ignores soil information, leading to the lower accuracy of the classification results (Wei et al., 2018). The multiple indicator method includes two categories: use of the feature space models for classification, and use of machine learning methods to construct evaluation models and classification criteria. Common feature space models include the albedo-normalized differen tialvegetation index (NDVI) model (Wei et al., 2020) and point-point model (Guo et al., 2020). Because such models only consider two in dicators,it is difficult to accurately monitor complex desertificatio information (Duan et al., 2019). In the common multiple indicator classification methods based on machine learning methods, the in dicatorsmainly include the NDVI, albedo, topsoil grain size index (TGSI), and land surface temperature.\nThe spatio temporalcharacteristics of desertification in Mongolia in 1990–2020 and revealed its driving forces. Based on all Landsat images in 1990–2020 in Mongolia, extensive ground surveys, and the existing desertification classification system, we used decision tree, support vector machine (SVM), RF, naive Bayes (NB), minimum distance (MD), and maximum entropy (ME) classifiers, to extract desertification information for seven periods (1990, 1995, 2000, 2005, 2010, 2015, and 2020)."
  },
  {
    "objectID": "Week4.html#reflection",
    "href": "Week4.html#reflection",
    "title": "5  Week 4 - Policy",
    "section": "5.3 REFLECTION",
    "text": "5.3 REFLECTION\n**NEED TO UPDATE it\nIf i am a city planner how can we start to address this What areas should i start with (funding often limited) What stage in the planning process might this come Who would be responsible (e.g. what department) What skills do they need What benefits can this bring to the city (often finances!) How does this help the city align with global agendas What stakeholders do i need to consider\n\n\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green Wall, Africa.” https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\n“Koch Quoted in BBC Article on Dubai, Desertification.” n.d. https://www.maxwell.syr.edu/news/article/koch-quoted-in-bbc-article-on-dubai-desertification.\n\n\nStallwood, Paige. 2022. “Desertification in Africa: Causes, Effects and Solutions.” https://earth.org/desertification-in-africa/."
  },
  {
    "objectID": "Week5.html#summary",
    "href": "Week5.html#summary",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.1 SUMMARY",
    "text": "6.1 SUMMARY\nThis week we studied the much awaited Google Earth Engine. ” Google Earth Engine is a cloud-based platform that makes it easy to access high-performance computing resources for processing very large geospatial datasets. It is designed to help researchers easily disseminate their results to other researchers, policy makers, NGOs, field workers, and even the general public. Once an algorithm has been developed on Earth Engine, users can produce systematic data products or deploy interactive applications backed by Earth Engine’s resources, without needing to be an expert in application development, web programming or HTML.” (Gorelick et al. 2017)\nIt is accessed and controlled through an application programming interface (API) and an associated web-based interactive development environment (IDE) that enables rapid prototyping and visualization of results. So basically GEE can load large datasets within seconds instead of downloading the big heavy satellite imagery which we did during the first three weeks of our practicals. It consist of Data Catalog which all types of Satellite imagery.\nGEE has a concept of client and server side where the code which we put runs on the client side i.e. Browser so we do not run anything locally which speeds up the process. It uses Javascript as the programming Language.\nThe raster data is called image and the vector data is called feature.\nA stack of images is called Image Collection and Feature stack (lots of polygons) is known as Feature Collection.\n\n\n\nGEE Screen : (“Beginner’s Cookbook | Google Earth Engine,” n.d.)"
  },
  {
    "objectID": "Week5.html#application",
    "href": "Week5.html#application",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.2 APPLICATION",
    "text": "6.2 APPLICATION\n“GEE is free and allows users to write open-source code that can be run by others in one click, thereby yielding fully reproducible results. These features have put GEE on the cutting edge of scientific research. The following plot visualizes the number of journal articles conducted using different geospatial analysis software platforms” (Ballinger 2024)\n\nHere we can discuss some GEE applications along with several case studies\nVegetation-\nThe full archive of the Landsat imagery was processed within GEE to map the vegetation dynamics from 1988 to 2017 in Queensland, Australia (xie2019?). Field observations were utilized to evaluate the performance of the proposed algorithm and an overall accuracy of 82.6% was reported. The authors emphasized the high computational efficiency of GEE compared to when they did the same analysis using traditional methods. The Image below accounts for different applications where GEE is used.\n\n\n\nGEE Applications(Amani et al. 2020)\n\n\nLong et al. (Long et al. 2019) proposed an automatic method for producing a global annual burned area maps using all available Landsat images acquired between 2014 and 2015 within the GEE cloud computing platform."
  },
  {
    "objectID": "Week5.html#reflection",
    "href": "Week5.html#reflection",
    "title": "6  Week 5 - Google Earth Engine",
    "section": "6.3 REFLECTION",
    "text": "6.3 REFLECTION\nOverall this week was quite anticipated from so long as everyone was waiting to learn GEE and how to use this tool and finally when we got to work on it. it seems pretty easy and straightforward. I think the processing time is very fast as we were able to finish the practical in class otherwise if we compare it with SNAP or even R we first need to download and sort out the imagery to do the analysis which took quite some time. Today we learn how to use the aggregrate function like hoe to use the reduce function and calculate the mean, median and standard variation of the stack of images.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial Applications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\n“Beginner’s Cookbook | Google Earth Engine.” n.d. https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca Moore. 2017. “Google Earth Engine: Planetary-scale Geospatial Analysis for Everyone.” Remote Sensing of Environment, Big Remotely Sensed Data: Tools, applications and experiences, 202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang, Bingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin. 2019. “30 m Resolution Global Annual Burned Area Mapping Based on Landsat Images and Google Earth Engine.” Remote Sensing 11 (February): 489. https://doi.org/10.3390/rs11050489."
  },
  {
    "objectID": "Week7.html#summary",
    "href": "Week7.html#summary",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.1 SUMMARY",
    "text": "8.1 SUMMARY\nThere is some Pre-classified data which can be used where we don’t need to do the classification. For example Dynamic World Data which use SR for labelling but use TOA for the model. here we use Machine learning models like Convolution Neural Network. Accuracy is assessed through Confusion Matrix.\nWhen we use such classified data we check that how was it trained. There are some issues like blobbiness as its a 50x50 m image.\nOBJECT BASED IMAGE ANALYSIS (OBIA)\nHere we talk about pixels and distance mainly Euclidian distance. OBIA is similarity between pixels. It tries to match the similar pixels. for example - Take a RGB near-infrared 1m image. Its a typical kind of image where we want to do segmentation and OBIA. we do OBIA to do clean classification. Like if we classify on the mean of all the vectors inside a particular polygon we will get a solid result as opposed to classifying on individual pixels we will get a lot salt and pepper noise.\nThere comes also a question why do we need object based analysis? It is to improve in signal and noise. There are thousand pixels here so we might want to turn these things into objects.\n\n\n\n\n\n\n\n\nOriginal Image to Classify\nDifferent Crop Types\nPer Pixel Classification\n\n\n\n\n\n\n\n\n\n\nFig1: Process of Classification\nWe use mode reducer in neighborhood to clean the image. it will remove the single pixel outliers. we also talk about spectral gradient and can we use super-pixels to remove the salt and pepper noise.\n\nFig2: Spectral Gradient and Distances\nSNIC- (Super pixel non-iterative clustering) Seeded region growing\nThis makes clusters without using k-means. It uses a regular grid of points (like k-means) but then assigns pixels to points through distance color and co-ordinates - it represents normalised spatial and color distances. we also have seed grid which denotes spacing in pixel. we can set square or hex grid.\nImage.reduceConnectedComponents\n(Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the most common method regular points on the image work out spatial distance (from point to centre of pixel) = closeness to centre colour difference (RGB vs RGB to centre point) = homogenity of colours\nThe SLIC algorithm works iteratively, repeating the above process until it reaches the expected number of iterations.\nsub pixel analysis/ spectral mixture analysis\nSupervised Classification in Remote Sensing\nIn supervised classification, you select training samples and classify your image based on your chosen samples. Your training samples are key because they will determine which class each pixel inherits in your overall image.\n\nFig3: Classification and Validation(GISGeography 2016)\nThe good approach is to train and test split\n\nThis is simply holding back a % of the original data used to train the model to then test it at the end\n\n\nFig.4: Testing and Training the ML model\nWhat do you think of spatial autocorrelation in this sense? Is OBIA is dealing with spatial autocorrelation?\nIn ML model if we use spatial data and they have not considered spatial autocorrelation between its training and testing data then its considered too good or overfitted. To solve this we do spatial cross validation"
  },
  {
    "objectID": "Week7.html#application",
    "href": "Week7.html#application",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.2 APPLICATION",
    "text": "8.2 APPLICATION\nThe researchers lately have diverted themselves to a much simpler task of simplifying an image into a smaller cluster of pixels called super pixels than using traditional segmentation algorithms. Several applications such as object localization, multi-class segmentation, optical flow, body model estimation, object tracking, depth estimation took advantage of super pixels.\nThe algorithm to do this is Simple linear Iterative Clustering Algorithm(SLIC) but it has some limitations. It requires several iterations for the centroids to converge. It uses a distance map of the same size as the number of input pixels, which amounts to significant memory consumption for image stacks or video volumes. Lastly, SLIC enforces connectivity only as a post-processing step. So the better version of this is simple non-iterative clustering (SNIC). It is non-iterative, explicitly enforces connectivity, is computationally cheaper, uses lesser memory.(Achanta and Susstrunk 2017)\nSpatial autocorrelation is inherent to remotely sensed data. nearby pixels are more similar than distant ones. The evaluation of classification accuracy has always been considered as an important issue in the remote sensing community. (Congalton 1991)\nDespite the importance attached to the accuracy assessment protocol, a gap, sometimes a serious one, is often found between the performance metrics of a model and the real quality of the resulting map. This tends to reduce the confidence placed in accuracy statistics and to discredit the true capacity of remote sensing in the opinion of the t end users. Because of spatial autocorrelation, spectral values of close pixels are often more similar than those of distant ones, producing falsely high accuracy metrics if the sampling design is not used for testing. (Roberts et al. 2017)\nFrom a statistical point of view, two problems can arise with spatial autocorrelation: (1) spatial non-independence of the classification errors (or model residuals) and (2) spatial non-independence of the training and test sets used for accuracy assessment.\nThe six cross-validation strategies with different sample sizes were applied to the department of Herault-34. The classification performances obtained by cross-validation on Herault-34 are given in Fig. 5. Overall, the results show that ignoring dependence between training and test sets leads to very high accuracy metrics whatever the sample size. This is particularly clear at the pixel level but also at the object level with large samples. Compared to sampling strategies that account for spatial autocorrelation, the accuracy metrics are overestimated. Our results revealed notable underestimation of generalization errors when traditional non spatial approaches were used to assess the accuracy. Pixel-based samplings were the most affected. Object-based strategies mitigate the effect of spatial dependence since the pixels used for training and testing never belong to the same forest stands. Nonetheless, non-spatial data-splitting at the object level also leads to overestimation of predictive performance. we need to change practices in classification accuracy assessment using spatial imagery. A data splitting design ensuring spatial independence between the training and test sets should be the standard approach for validation. (Karasiak et al. 2022)\n\nFig.5: Average Overall accuracy based on the RF classifier"
  },
  {
    "objectID": "Week7.html#reflection",
    "href": "Week7.html#reflection",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.3 REFLECTION",
    "text": "8.3 REFLECTION\nThis week we discussed about object based image analysis i think is very important part of classification. The issues related to that and if for example we have spatial data then we always have spatial autocorrelation. They are kind of related . So we need to keep a check on that which can be resolved through spatial cross validation or object based image analysis. If we are using machine learning models for spatial data we need to take care of overfitting of data and how can we make the more generalized model which can be use for different imagery. this week also talks about lots of methods we discussed in GIS module like Moran’s I, K-means Clustering and GWR. So here we see the merge of Remote Sensing and GIS.\n\n\n\n\nAchanta, Radhakrishna, and Sabine Susstrunk. 2017. “2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).” In, 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data.” Remote Sensing of Environment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nGISGeography. 2016. “Supervised and Unsupervised Classification in Remote Sensing.” https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Achanta, Radhakrishna, and Sabine Susstrunk. 2017. “2017 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR).”\nIn, 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing Platform for\nRemote Sensing Big Data Applications: A Comprehensive Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nAyadi, Souha, and Zied Lachiri. 2022. “Deep Neural Network for\nVisual Emotion Recognition Based on ResNet50 Using Song-Speech\nCharacteristics.” 2022 5th International Conference on\nAdvanced Systems and Emergent Technologies (IC_ASET),\nMarch, 363–68. https://doi.org/10.1109/IC_ASET53395.2022.9765898.\n\n\nBallinger, Dr Ollie. 2024. “CASA0025: Building Spatial\nApplications with Big Data.” https://oballinger.github.io/CASA0025/.\n\n\n“Beginner’s Cookbook | Google Earth Engine.” n.d. https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook.\n\n\nBhatia, Navnina. 2019. “What Is Out of Bag (OOB) Score in Random\nForest?” https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of\nClassifications of Remotely Sensed Data.” Remote Sensing of\nEnvironment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\n“Decision Tree Tutorials & Notes | Machine Learning.”\nn.d. https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/.\n\n\nGISGeography. 2016. “Supervised and Unsupervised Classification in\nRemote Sensing.” https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nGorelick, Noel, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David\nThau, and Rebecca Moore. 2017. “Google Earth Engine:\nPlanetary-scale Geospatial Analysis for\nEveryone.” Remote Sensing of Environment, Big\nRemotely Sensed Data: Tools, applications and experiences,\n202 (December): 18–27. https://doi.org/10.1016/j.rse.2017.06.031.\n\n\nJose, Tanya. 2023. “Project in-Depth: The Great Green Wall,\nAfrica.” https://www.re-thinkingthefuture.com/materials-construction/a11157-project-in-depth-the-great-green-wall-africa/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022.\n“Spatial Dependence Between Training and Test Sets: Another\nPitfall of Classification Accuracy Assessment in Remote Sensing.”\nMachine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\n“Koch Quoted in BBC Article on Dubai, Desertification.”\nn.d. https://www.maxwell.syr.edu/news/article/koch-quoted-in-bbc-article-on-dubai-desertification.\n\n\nLong, Tengfei, Zhaoming Zhang, Guojin He, Weili Jiao, Chao Tang,\nBingfang Wu, Xiaomei Zhang, Wang guizhou Wang guizhou, and Ranyu Yin.\n2019. “30 m Resolution Global Annual Burned Area Mapping Based on\nLandsat Images and Google Earth Engine.” Remote Sensing\n11 (February): 489. https://doi.org/10.3390/rs11050489.\n\n\nMerry, Krista, Jacek Siry, Pete Bettinger, and J. M. Bowker. 2014.\n“Urban Tree Cover Change in Detroit and Atlanta, USA,\n19512010.” Cities 41 (December): 123–31. https://doi.org/10.1016/j.cities.2014.06.012.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith,\nGurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017.\n“Cross-Validation Strategies for Data with Temporal,\nSpatial, Hierarchical, or Phylogenetic Structure.”\nEcography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881.\n\n\nRustamov, Jaloliddin, Zahiriddin Rustamov, and Nazar Zaki. 2023.\n“Green Space Quality Analysis Using Machine Learning\nApproaches.” Sustainability 15 (10): 7782. https://doi.org/10.3390/su15107782.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Yue lin, Muye\nGan, Ke Wang, Arunima Malik, and George Alan Blackburn. 2021.\n“Remote Sensing of Urban Green Spaces: A Review.” Urban\nForestry & Urban Greening 57 (January): 126946. https://doi.org/10.1016/j.ufug.2020.126946.\n\n\nSingh, Seema. 2018. “Understanding the Bias-Variance\nTradeoff.” https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229.\n\n\nStallwood, Paige. 2022. “Desertification in Africa: Causes,\nEffects and Solutions.” https://earth.org/desertification-in-africa/.\n\n\nSun, Yi, Xingzhi Wang, Jiayin Zhu, Liangjian Chen, Yuhang Jia, Jean M.\nLawrence, Luo-hua Jiang, Xiaohui Xie, and Jun Wu. 2021. “Using\nMachine Learning to Examine Street Green Space Types at a High Spatial\nResolution: Application in Los Angeles County on Socioeconomic\nDisparities in Exposure.” The Science of the Total\nEnvironment 787 (September): 147653. https://doi.org/10.1016/j.scitotenv.2021.147653.\n\n\nTariq, Aqil, Jianguo Yan, Alexandre S. Gagnon, Mobushir Riaz Khan, and\nFaisal Mumtaz. 2023. “Mapping of Cropland, Cropping Patterns and\nCrop Types by Combining Optical Remote Sensing Images with Decision Tree\nClassifier and Random Forest.” Geo-Spatial Information\nScience 26 (3): 302–20. https://doi.org/10.1080/10095020.2022.2100287.\n\n\n“What Is a Knowledge-Based System in the Context of Artificial\nIntelligence?” n.d. https://www.quora.com/What-is-a-knowledge-based-system-in-the-context-of-artificial-intelligence.\n\n\nZhang, C., and F. Qiu. 2012. “Mapping Individual Tree Species in\nan Urban Forest Using Airborne Lidar Data and Hyperspectral Imagery: AAG\nRemote Sensing Specialty Group 2011 Award Winner.”\nPhotogrammetric Engineering and Remote Sensing 78 (10):\n1079–87. https://doi.org/10.14358/PERS.78.10.1079."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Learning Diary",
    "section": "",
    "text": "1 ABOUT\nThis is my online Learning diary.\nIn CASA0023 we are asked to make a portfolio that will help us in the future.\nHi! I am Sameera. I am from India and have lived in many different parts of the country. When someone ask me where in India you are from i have to think and reply as i was born in Kanpur, completed my school there and for further studies I moved to Delhi, the capital of India. and Past five years i have been moving to different cities from Ahmedabad to Kochi to Bangalore. Now here i am in London.\nReading maps has fascinated me ever since I attended cartography lessons back in school; however, I never knew that I would find my calling in the same field in the future. It is exhilarating how I followed my passion and entered the field of GIS after spending over six years working as a lecturer at an undergraduate college.\nI love travelling. My favorite destination till date is Leh, Ladakh. Here’s the picture from that place."
  },
  {
    "objectID": "Week8.html#summary",
    "href": "Week8.html#summary",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.1 SUMMARY",
    "text": "8.1 SUMMARY\nThere is some Pre-classified data which can be used where we don’t need to do the classification. For example Dynamic World Data which use SR for labelling but use TOA for the model. here we use Machine learning models like Convolution Neural Network. Accuracy is assessed through Confusion Matrix.\nWhen we use such classified data we check that how was it trained. There are some issues like blobbiness as its a 50x50 m image.\nOBJECT BASED IMAGE ANALYSIS (OBIA)\nHere we talk about pixels and distance mainly Euclidian distance. OBIA is similarity between pixels. It tries to match the similar pixels. for example - Take a RGB near-infrared 1m image. Its a typical kind of image where we want to do segmentation and OBIA. we do OBIA to do clean classification. Like if we classify on the mean of all the vectors inside a particular polygon we will get a solid result as opposed to classifying on individual pixels we will get a lot salt and pepper noise.\nThere comes also a question why do we need object based analysis? It is to improve in signal and noise. There are thousand pixels here so we might want to turn these things into objects.\n\n\n\n\n\n\n\n\nOriginal Image to Classify\nDifferent Crop Types\nPer Pixel Classification\n\n\n\n\n\n\n\n\n\n\nFig1: Process of Classification\nWe use mode reducer in neighborhood to clean the image. it will remove the single pixel outliers. we also talk about spectral gradient and can we use super-pixels to remove the salt and pepper noise.\n\nFig2: Spectral Gradient and Distances\nSNIC- (Super pixel non-iterative clustering) Seeded region growing\nThis makes clusters without using k-means. It uses a regular grid of points (like k-means) but then assigns pixels to points through distance color and co-ordinates - it represents normalised spatial and color distances. we also have seed grid which denotes spacing in pixel. we can set square or hex grid.\nImage.reduceConnectedComponents\n(Simple Linear Iterative Clustering) Algorithm for Superpixel generation is the most common method regular points on the image work out spatial distance (from point to centre of pixel) = closeness to centre colour difference (RGB vs RGB to centre point) = homogenity of colours\nThe SLIC algorithm works iteratively, repeating the above process until it reaches the expected number of iterations.\nsub pixel analysis/ spectral mixture analysis\nSupervised Classification in Remote Sensing\nIn supervised classification, you select training samples and classify your image based on your chosen samples. Your training samples are key because they will determine which class each pixel inherits in your overall image.\n\nFig3: Classification and Validation(GISGeography 2016)\nThe good approach is to train and test split\n\nThis is simply holding back a % of the original data used to train the model to then test it at the end\n\n\nFig.4: Testing and Training the ML model\nWhat do you think of spatial autocorrelation in this sense? Is OBIA is dealing with spatial autocorrelation?\nIn ML model if we use spatial data and they have not considered spatial autocorrelation between its training and testing data then its considered too good or overfitted. To solve this we do spatial cross validation"
  },
  {
    "objectID": "Week8.html#application",
    "href": "Week8.html#application",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.2 APPLICATION",
    "text": "8.2 APPLICATION\nThe researchers lately have diverted themselves to a much simpler task of simplifying an image into a smaller cluster of pixels called super pixels than using traditional segmentation algorithms. Several applications such as object localization, multi-class segmentation, optical flow, body model estimation, object tracking, depth estimation took advantage of super pixels.\nThe algorithm to do this is Simple linear Iterative Clustering Algorithm(SLIC) but it has some limitations. It requires several iterations for the centroids to converge. It uses a distance map of the same size as the number of input pixels, which amounts to significant memory consumption for image stacks or video volumes. Lastly, SLIC enforces connectivity only as a post-processing step. So the better version of this is simple non-iterative clustering (SNIC). It is non-iterative, explicitly enforces connectivity, is computationally cheaper, uses lesser memory.(Achanta and Susstrunk 2017)\nSpatial autocorrelation is inherent to remotely sensed data. nearby pixels are more similar than distant ones. The evaluation of classification accuracy has always been considered as an important issue in the remote sensing community. (Congalton 1991)\nDespite the importance attached to the accuracy assessment protocol, a gap, sometimes a serious one, is often found between the performance metrics of a model and the real quality of the resulting map. This tends to reduce the confidence placed in accuracy statistics and to discredit the true capacity of remote sensing in the opinion of the t end users. Because of spatial autocorrelation, spectral values of close pixels are often more similar than those of distant ones, producing falsely high accuracy metrics if the sampling design is not used for testing. (Roberts et al. 2017)\nFrom a statistical point of view, two problems can arise with spatial autocorrelation: (1) spatial non-independence of the classification errors (or model residuals) and (2) spatial non-independence of the training and test sets used for accuracy assessment.\nThe six cross-validation strategies with different sample sizes were applied to the department of Herault-34. The classification performances obtained by cross-validation on Herault-34 are given in Fig. 5. Overall, the results show that ignoring dependence between training and test sets leads to very high accuracy metrics whatever the sample size. This is particularly clear at the pixel level but also at the object level with large samples. Compared to sampling strategies that account for spatial autocorrelation, the accuracy metrics are overestimated. Our results revealed notable underestimation of generalization errors when traditional non spatial approaches were used to assess the accuracy. Pixel-based samplings were the most affected. Object-based strategies mitigate the effect of spatial dependence since the pixels used for training and testing never belong to the same forest stands. Nonetheless, non-spatial data-splitting at the object level also leads to overestimation of predictive performance. we need to change practices in classification accuracy assessment using spatial imagery. A data splitting design ensuring spatial independence between the training and test sets should be the standard approach for validation. (Karasiak et al. 2022)\n\nFig.5: Average Overall accuracy based on the RF classifier"
  },
  {
    "objectID": "Week8.html#reflection",
    "href": "Week8.html#reflection",
    "title": "8  Week 7 - Classification- The Big Question",
    "section": "8.3 REFLECTION",
    "text": "8.3 REFLECTION\nThis week we discussed about object based image analysis i think is very important part of classification. The issues related to that and if for example we have spatial data then we always have spatial autocorrelation. They are kind of related . So we need to keep a check on that which can be resolved through spatial cross validation or object based image analysis. If we are using machine learning models for spatial data we need to take care of overfitting of data and how can we make the more generalized model which can be use for different imagery. this week also talks about lots of methods we discussed in GIS module like Moran’s I, K-means Clustering and GWR. So here we see the merge of Remote Sensing and GIS.\n\n\n\n\nAchanta, Radhakrishna, and Sabine Susstrunk. 2017. “2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).” In, 4895–4904. Honolulu, HI: IEEE. https://doi.org/10.1109/CVPR.2017.520.\n\n\nCongalton, Russell G. 1991. “A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data.” Remote Sensing of Environment 37 (1): 35–46. https://doi.org/10.1016/0034-4257(91)90048-B.\n\n\nGISGeography. 2016. “Supervised and Unsupervised Classification in Remote Sensing.” https://gisgeography.com/supervised-unsupervised-classification-arcgis/.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nRoberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017. “Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.” Ecography 40 (8): 913–29. https://doi.org/10.1111/ecog.02881."
  },
  {
    "objectID": "Week9.html#summary",
    "href": "Week9.html#summary",
    "title": "9  Week 9- Synthetic Aperture Radar (SAR) Data",
    "section": "9.1 SUMMARY",
    "text": "9.1 SUMMARY\nToday final topic was on Synthetic Aperture Radar (SAR) data\nSAR is an active sensor which means a sensor which provides their own illumination. for example an active sensor sees the world as the way bats do as they emit a chirp and listen to the backscatter of an echo if something gets reflected while passive sensor (optical) sees the world as our eyes do or camera do.\nRadar sensor picks up the signal reflected backscattered from the earth surface. They check the topology of the surface based on how it reflects.\nInterpreting Radar Images\n\nRegions of calm water and other smooth surfaces appear black, because the radar pulse reflects away from the spacecraft\nRough surfaces appear brighter, as they reflect the radar in all directions, and more of the energy is scattered back to the antenna. \n\nThey see through weather and clouds. Different wavelengths penetrates through different surfaces used in different applications. So choice of wavelenghth is important.\n\n(Earth Science Data Systems 2020)\nThe most commonly used Band in imaging is C-band with 4-8 GHz frequency.\n** SAR Polarization **\nSo we have vertical and horizontal transmission and reflection.\n\n\n\n\n\nHorizontally and vertically polarized radar signal.(Dabboor et al. 2018)"
  },
  {
    "objectID": "Week9.html#application",
    "href": "Week9.html#application",
    "title": "9  Week 9- Synthetic Aperture Radar (SAR) Data",
    "section": "9.2 APPLICATION",
    "text": "9.2 APPLICATION\nSAR is used in tracking flooding. Area that is flooded appear darker."
  },
  {
    "objectID": "Week9.html#reflection",
    "href": "Week9.html#reflection",
    "title": "9  Week 9- Synthetic Aperture Radar (SAR) Data",
    "section": "9.3 REFLECTION",
    "text": "9.3 REFLECTION\n\n\n\n\nDabboor, Mohammed, Brian Brisco, Mohammed Dabboor, and Brian Brisco. 2018. “Wetland Monitoring and Mapping Using Synthetic Aperture Radar.” In. IntechOpen. https://doi.org/10.5772/intechopen.80224.\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar."
  }
]